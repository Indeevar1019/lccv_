{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import openml\n",
    "import lccv\n",
    "import os, psutil\n",
    "import gc\n",
    "import logging\n",
    "import traceback\n",
    "\n",
    "from func_timeout import func_timeout, FunctionTimedOut\n",
    "\n",
    "import time\n",
    "import random\n",
    "\n",
    "import itertools as it\n",
    "import scipy.stats\n",
    "from scipy.sparse import lil_matrix\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "import sklearn\n",
    "from sklearn import metrics\n",
    "from sklearn import *\n",
    "\n",
    "from func_timeout import func_timeout, FunctionTimedOut\n",
    "#from commons import *\n",
    "from tqdm import tqdm\n",
    "\n",
    "import import_ipynb\n",
    "import Commons\n",
    "import copy\n",
    "\n",
    "eval_logger = logging.getLogger(\"evalutils\")\n",
    "\n",
    "\n",
    "def get_dataset(openmlid):\n",
    "    ds = openml.datasets.get_dataset(openmlid)\n",
    "    df = ds.get_data()[0]\n",
    "    num_rows = len(df)\n",
    "        \n",
    "    # prepare label column as numpy array\n",
    "    print(f\"Read in data frame. Size is {len(df)} x {len(df.columns)}.\")\n",
    "    X = np.array(df.drop(columns=[ds.default_target_attribute]).values)\n",
    "    y = np.array(df[ds.default_target_attribute].values)\n",
    "    if y.dtype != int:\n",
    "        y_int = np.zeros(len(y)).astype(int)\n",
    "        vals = np.unique(y)\n",
    "        for i, val in enumerate(vals):\n",
    "            mask = y == val\n",
    "            y_int[mask] = i\n",
    "        y = y_int\n",
    "        \n",
    "    print(f\"Data is of shape {X.shape}.\")\n",
    "    return X, y\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def format_learner(learner):\n",
    "    learner_name = str(learner).replace(\"\\n\", \" \").replace(\"\\t\", \" \")\n",
    "    for k in  range(20):\n",
    "        learner_name = learner_name.replace(\"  \", \" \")\n",
    "    return learner_name\n",
    "\n",
    "\n",
    "def decide_block_train(pl, anchor):\n",
    "    steps = pl.steps\n",
    "    predictor = steps[-1][1]\n",
    "    if type(predictor) == sklearn.ensemble.HistGradientBoostingClassifier:\n",
    "        min_samples_leaf = predictor.min_samples_leaf\n",
    "        return anchor >= 2* min_samples_leaf\n",
    "    \n",
    "    return True\n",
    "    \n",
    "class Evaluator:\n",
    "    \n",
    "    def __init__(self, X, y, binarize_sparse = False):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.best_observations=None\n",
    "        # determine fixed pre-processing steps for imputation and binarization\n",
    "        types = [set([type(v) for v in r]) for r in X.T]\n",
    "        numeric_features = [c for c, t in enumerate(types) if len(t) == 1 and list(t)[0] != str]\n",
    "        numeric_transformer = Pipeline([(\"imputer\", sklearn.impute.SimpleImputer(strategy=\"median\"))])\n",
    "        categorical_features = [i for i in range(X.shape[1]) if i not in numeric_features]\n",
    "        missing_values_per_feature = np.sum(pd.isnull(X), axis=0)\n",
    "        eval_logger.info(f\"There are {len(categorical_features)} categorical features, which will be binarized.\")\n",
    "        eval_logger.info(f\"Missing values for the different attributes are {missing_values_per_feature}.\")\n",
    "        if len(categorical_features) > 0 or sum(missing_values_per_feature) > 0:\n",
    "            categorical_transformer = Pipeline([\n",
    "                (\"imputer\", sklearn.impute.SimpleImputer(strategy=\"most_frequent\")),\n",
    "                (\"binarizer\", sklearn.preprocessing.OneHotEncoder(handle_unknown='ignore', sparse = binarize_sparse)),\n",
    "            ])\n",
    "            self.mandatory_pre_processing = [(\"impute_and_binarize\", ColumnTransformer(\n",
    "                transformers=[\n",
    "                    (\"num\", numeric_transformer, numeric_features),\n",
    "                    (\"cat\", categorical_transformer, categorical_features),\n",
    "                ]\n",
    "            ))]\n",
    "        else:\n",
    "            self.mandatory_pre_processing = []\n",
    "    \n",
    "    def eval_pipeline_on_fold(self, pl, X_train, X_test, y_train, y_test, timeout = None):\n",
    "        try:\n",
    "            pl = Pipeline(self.mandatory_pre_processing + sklearn.base.clone(pl).steps)\n",
    "            \n",
    "            h1_before, h2_before = hash(X_train.tostring()), hash(X_test.tostring())\n",
    "            if timeout is None:\n",
    "                eval_logger.info(f\"Fitting model with {X_train.shape[0]} instances and without timeout.\")\n",
    "                pl.fit(X_train, y_train)\n",
    "            else:\n",
    "                eval_logger.info(f\"Fitting model with {X_train.shape[0]} instances and timeout {timeout}.\")\n",
    "                func_timeout(timeout, pl.fit, (X_train, y_train))\n",
    "                \n",
    "            y_hat = pl.predict(X_test)\n",
    "            error_rate = 1 - sklearn.metrics.accuracy_score(y_test, y_hat)\n",
    "            eval_logger.info(f\"Observed an error rate of {error_rate}\")\n",
    "            h1_after, h2_after = hash(X_train.tostring()), hash(X_test.tostring())\n",
    "            if h1_before != h1_after or h2_before != h2_after:\n",
    "                raise Exception(\"Pipeline has modified the original data, which is forbidden!\")\n",
    "            return error_rate\n",
    "        \n",
    "        except FunctionTimedOut:\n",
    "            eval_logger.info(f\"Timeout observed for evaluation, stopping and returning nan.\")\n",
    "        except KeyboardInterrupt:\n",
    "            raise\n",
    "        except Exception as e:\n",
    "            eval_logger.info(f\"Observed some exception. Stopping. Exception: {e}\")\n",
    "        \n",
    "        return np.nan\n",
    "    \n",
    "    def mccv(self, learner, target_size=.9, timeout=None, seed=0, repeats = 10):\n",
    "\n",
    "        \"\"\"\n",
    "        Conducts a 90/10 MCCV (imitating a bit a 10-fold cross validation)\n",
    "        \"\"\"\n",
    "        eval_logger.info(f\"Running mccv with seed  {seed}\")\n",
    "        if not timeout is None:\n",
    "            deadline = time.time() + timeout\n",
    "\n",
    "        scores = []\n",
    "        n = self.X.shape[0]\n",
    "        num_examples = int(target_size * n)\n",
    "        deadline = None if timeout is None else time.time() + timeout\n",
    "\n",
    "        seed *= 13\n",
    "        for r in range(repeats):\n",
    "            eval_logger.info(f\"Seed in MCCV: {seed}. Training on {num_examples} examples. That is {np.round(100 * num_examples / self.X.shape[0])}% of the data (testing on rest).\")\n",
    "            \n",
    "            # get random train/test split based on seed\n",
    "            random.seed(seed)\n",
    "            n = self.X.shape[0]\n",
    "            indices_train = random.sample(range(n), num_examples)\n",
    "            mask_train = np.zeros(n)\n",
    "            mask_train[indices_train] = 1\n",
    "            mask_train = mask_train.astype(bool)\n",
    "            mask_test = (1 - mask_train).astype(bool)\n",
    "            X_train = self.X[mask_train]\n",
    "            y_train = self.y[mask_train]\n",
    "            X_test = self.X[mask_test]\n",
    "            y_test = self.y[mask_test]\n",
    "            \n",
    "            # evaluate pipeline\n",
    "            timeout_local = None if timeout is None else deadline - time.time()\n",
    "            error_rate = self.eval_pipeline_on_fold(learner, X_train, X_test, y_train, y_test, timeout=timeout_local)\n",
    "            scores.append(error_rate)\n",
    "            seed += 1\n",
    "            del X_train, X_test\n",
    "        gc.collect()\n",
    "\n",
    "        return scores\n",
    "    \n",
    "    def get_result_of_cv(self,learner_inst ,folds, seed = None, timeout = None):\n",
    "        kf = sklearn.model_selection.KFold(n_splits=folds, random_state=np.random.RandomState(seed), shuffle=True)\n",
    "        scores = []\n",
    "        deadline = time.time() + timeout if timeout is not None else None\n",
    "        for train_index, test_index in kf.split(X):\n",
    "            X_train, y_train = X[train_index], y[train_index]\n",
    "            X_test, y_test = X[test_index], y[test_index]\n",
    "            timeout_loc = None if timeout is None else deadline - time.time()\n",
    "            error_rate = self.eval_pipeline_on_fold(learner_inst, X_train, X_test, y_train, y_test, timeout = timeout_loc)\n",
    "            if not np.isnan(error_rate):\n",
    "                scores.append(error_rate)\n",
    "        out = np.mean(scores) if scores else np.nan\n",
    "        eval_logger.info(f\"Returning {out} as the avg over observed scores {scores}\")\n",
    "        return out\n",
    "    \n",
    "\n",
    "    def get_pipeline_from_descriptor(self, learner):\n",
    "        return learner\n",
    "        #return sklearn.pipeline.Pipeline([(step_name, build_estimator(comp, params, self.X, self.y)) for step_name, (comp, params) in learner])\n",
    "\n",
    "    '''\n",
    "        This is the main function that must be implemented by the approaches\n",
    "    '''\n",
    "    def select_model(self, learners):\n",
    "        raise NotImplemented()\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "class SH(Evaluator):\n",
    "    \n",
    "    def __init__(self, X, y, binarize_sparse, timeout_per_evaluation, max_train_budget, b_min = 64, seed = 0, repeats = 10):\n",
    "        self.timeout_per_evaluation = timeout_per_evaluation\n",
    "        self.b_min = b_min\n",
    "        self.seed = seed\n",
    "        self.repeats = repeats\n",
    "        self.max_train_budget = max_train_budget\n",
    "        self.r=0\n",
    "        super().__init__(X, y, binarize_sparse)\n",
    "    \n",
    "    def select_model(self, learners):\n",
    "        b_min = self.b_min\n",
    "        test_budget = 1 - self.max_train_budget\n",
    "        b_max = int(self.X.shape[0] * (1 - test_budget))\n",
    "        timeout = self.timeout_per_evaluation\n",
    "        print(f\"b_max is {b_max}\")\n",
    "        n = len(learners)\n",
    "        num_phases = int(np.log2(n) - 1)\n",
    "        eta = (b_max / b_min)**(1/num_phases)\n",
    "        print(f\"Eta is {eta}\")\n",
    "        anchors = [int(np.round(b_min * eta**i)) for i in range(num_phases + 1)]\n",
    "        populations = [int(np.round(n / (2**i))) for i in range(num_phases + 1)]\n",
    "        if num_phases != int(num_phases):\n",
    "            raise Exception(f\"Number of learners is {len(learners)}, which is not a power of 2!\")\n",
    "        num_phases = int(num_phases)\n",
    "        print(f\"There will be {num_phases + 1} phases with the following setup.\")\n",
    "        for anchor, population in zip(anchors, populations):\n",
    "            print(f\"Evaluate {population} on {anchor}\")\n",
    "\n",
    "        best_seen_score = np.inf\n",
    "        best_seen_pl = None\n",
    "\n",
    "        def get_scores_on_budget(candidates, budget):\n",
    "            scores = []\n",
    "            for candidate in tqdm(candidates):\n",
    "                deadline = None if timeout is None else time.time() + timeout\n",
    "\n",
    "                temp_pipe = self.get_pipeline_from_descriptor(candidate)\n",
    "                scores_for_candidate_at_budget = []\n",
    "                for i in range(self.repeats):\n",
    "                    if deadline < time.time():\n",
    "                        break\n",
    "                    try:\n",
    "                        X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(self.X, self.y, train_size = budget, test_size = test_budget)\n",
    "                        error_rate = self.eval_pipeline_on_fold(temp_pipe, X_train, X_test, y_train, y_test, deadline - time.time())\n",
    "                        if not np.isnan(error_rate):\n",
    "                            scores_for_candidate_at_budget.append(np.round(error_rate, 4))\n",
    "                        else:\n",
    "                            scores_for_candidate_at_budget.append(np.nan)\n",
    "                    except KeyboardInterrupt:\n",
    "                        raise\n",
    "                    except Exception as e:\n",
    "                        print(f\"There was an error in the evaluation of candidate {candidate}. Ignoring it. Error: {e}\")\n",
    "                        scores_for_candidate_at_budget.append(np.nan)\n",
    "                \n",
    "                scores.append(scores_for_candidate_at_budget)\n",
    "            print(scores)\n",
    "            return scores\n",
    "\n",
    "        time_start = time.time()\n",
    "        #population = learners.copy()\n",
    "        population=copy.deepcopy(learners)\n",
    "        for i, anchor in enumerate(anchors):\n",
    "            time_start_phase = time.time()\n",
    "            scores_in_round = get_scores_on_budget(population, anchor)\n",
    "            runtime_phase = time.time() - time_start_phase\n",
    "            mean_scores_tmp = [np.nanmean(s) if np.count_nonzero(np.isnan(s)) < len(s) else np.nan for s in scores_in_round]\n",
    "            if all(np.isnan(mean_scores_tmp)):\n",
    "                print(\"All candidates evalated nan in last round, aborting evaluation.\")\n",
    "                break\n",
    "            mean_scores = mean_scores_tmp\n",
    "            index_of_best_mean_score_in_round = np.nanargmin(mean_scores)\n",
    "            best_mean_score_in_round = mean_scores[index_of_best_mean_score_in_round]\n",
    "            if best_mean_score_in_round < best_seen_score:\n",
    "                best_seen_score = best_mean_score_in_round\n",
    "                best_seen_pl = population[index_of_best_mean_score_in_round]\n",
    "\n",
    "            print(f\"Finished round {i+1} after {np.round(runtime_phase, 2)}s. Scores are: {mean_scores}.\\nBest score was: {best_mean_score_in_round} (all times best score was {best_seen_score})\")\n",
    "            best_indices = np.argsort(mean_scores)[:int(len(population) / 2)]\n",
    "            print(f\"Best indices are: {best_indices}.\")\n",
    "            if len(population) > 2:\n",
    "                population = [p for j, p in enumerate(population) if j in best_indices]\n",
    "        runtime = time.time () - time_start\n",
    "\n",
    "        return self.get_pipeline_from_descriptor(best_seen_pl)\n",
    "    \n",
    "\n",
    "class VerticalEvaluator(Evaluator):\n",
    "    \n",
    "    def __init__(self, X, y, binarize_sparse, validation, train_size, timeout_per_evaluation, epsilon, seed=0, exception_on_failure=False, other_args = {},best_observations=None):\n",
    "        super().__init__(X, y, binarize_sparse)\n",
    "        self.r=0\n",
    "        self.best_observations = best_observations\n",
    "        self.other_args = other_args\n",
    "        if validation == \"cv\":\n",
    "            if train_size == 0.8:\n",
    "                num_folds = 5\n",
    "            elif train_size == 0.9:\n",
    "                num_folds = 10\n",
    "            else:\n",
    "                raise ValueError(f\"Cannot run cross-validation for train_size {train_size}. Must be 0.8 or 0.9.\")\n",
    "            self.validation_func = lambda pl, seed: self.cv(pl, seed, num_folds, *self.other_args)\n",
    "        elif \"lccv\" in validation:\n",
    "            \n",
    "            is_flex = \"flex\" in validation\n",
    "            \n",
    "            self.r = 1.0\n",
    "            if train_size == 0.8:\n",
    "                self.validation_func = self.lccv80flex if is_flex else self.lccv80\n",
    "            elif train_size == 0.9:\n",
    "                self.validation_func = self.lccv90flex if is_flex else self.lccv90\n",
    "            else:\n",
    "                raise ValueError(f\"Cannot run LCCV for train_size {train_size}. Must be 0.8 or 0.9.\")\n",
    "        elif validation == \"wilcoxon\":\n",
    "            self.r = 1.0\n",
    "            self.best_observations = None\n",
    "            if train_size == 0.8:\n",
    "                self.validation_func = lambda pl, seed: self.wilcoxon(pl, seed = seed, folds = 5)\n",
    "            elif train_size == 0.9:\n",
    "                self.validation_func = lambda pl, seed: self.wilcoxon(pl, seed = seed, folds = 10)\n",
    "            else:\n",
    "                raise ValueError(f\"Cannot run Wilcoxon for train_size {train_size}. Must be 0.8 or 0.9.\")\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported validation function {validation}.\")\n",
    "        self.timeout_per_evaluation = timeout_per_evaluation\n",
    "        self.epsilon = epsilon\n",
    "        self.seed = seed\n",
    "        self.exception_on_failure = exception_on_failure\n",
    "        \n",
    "    def cv(self, pl, seed, folds):\n",
    "        kf = sklearn.model_selection.KFold(n_splits=folds, random_state=np.random.RandomState(seed), shuffle=True)\n",
    "        scores = []\n",
    "        deadline = time.time() + self.timeout_per_evaluation if self.timeout_per_evaluation is not None else None\n",
    "        for train_index, test_index in kf.split(self.X):\n",
    "            learner_inst_copy = sklearn.base.clone(pl)\n",
    "            X_train, y_train = self.X[train_index], self.y[train_index]\n",
    "            X_test, y_test = self.X[test_index], self.y[test_index]\n",
    "            timeout_loc = None if deadline is None else deadline - time.time()\n",
    "            scores.append(self.eval_pipeline_on_fold(pl, X_train, X_test, y_train, y_test, timeout = timeout_loc))\n",
    "        require_at_least_two = time.time() < deadline\n",
    "        is_valid_result = len(scores) > 0 and ((not require_at_least_two) or np.count_nonzero(np.isnan(scores)) < folds - 1)\n",
    "        out = np.nanmean(scores) if is_valid_result else np.nan # require at least two valid samples in the batch if the timeout was not hit\n",
    "        eval_logger.info(f\"Returning {out} as the avg over observed scores {scores}\")\n",
    "        return out\n",
    "    \n",
    "    def wilcoxon(self, pl, seed=0, folds = 10):\n",
    "\n",
    "        eval_logger.info(f\"Running Wilcoxon-guarded CV with seed  {seed}\")\n",
    "        if not self.timeout_per_evaluation is None:\n",
    "            deadline = time.time() + self.timeout_per_evaluation\n",
    "        \n",
    "        kf = sklearn.model_selection.KFold(n_splits=folds, random_state=np.random.RandomState(seed), shuffle=True)\n",
    "        scores = []\n",
    "        deadline = time.time() + self.timeout_per_evaluation if self.timeout_per_evaluation is not None else None\n",
    "        for inner_run, (train_index, test_index) in enumerate(kf.split(self.X)):\n",
    "            learner_inst_copy = sklearn.base.clone(pl)\n",
    "            X_train, y_train = self.X[train_index], self.y[train_index]\n",
    "            X_test, y_test = self.X[test_index], self.y[test_index]\n",
    "            timeout_loc = None if deadline is None else deadline - time.time()\n",
    "            scores.append(self.eval_pipeline_on_fold(pl, X_train, X_test, y_train, y_test, timeout = timeout_loc))\n",
    "\n",
    "            # now conduct a wilcoxon signed rank test to determine whether significance has been reached\n",
    "            scores_currently_best = np.array(self.best_observations[:len(scores)]) if self.best_observations is not None else np.ones(len(scores))\n",
    "            eval_logger.info(f\"Currently best observations after {inner_run + 1} evaluations: {np.round(scores_currently_best, 2)}\")\n",
    "            eval_logger.info(f\"Current cand.  observations after {inner_run + 1} evaluations: {np.round(scores, 2)}\")\n",
    "            if any(np.array(scores) != scores_currently_best):\n",
    "                statistic, pval = scipy.stats.wilcoxon(scores, scores_currently_best)\n",
    "                eval_logger.info(f\"p-value is {pval}\")\n",
    "                if pval < 0.05:\n",
    "                    eval_logger.info(f\"reached certainty in fold {inner_run + 1}.\")\n",
    "                    if np.mean(scores) > self.r:\n",
    "                        eval_logger.info(\"it is certainly worse, so aborting.\")\n",
    "                        break\n",
    "                    else:\n",
    "                        eval_logger.info(\"it is certainly better, so continuing\")\n",
    "            else:\n",
    "                eval_logger.info(\"omitting test, because all scores are still identical\")\n",
    "        require_at_least_two = time.time() < deadline\n",
    "        is_valid_result = len(scores) > 0 and ((not require_at_least_two) or np.count_nonzero(np.isnan(scores)) < folds - 1)\n",
    "        out = np.nanmean(scores) if is_valid_result else np.nan # require at least two valid samples in the batch if the timeout was not hit\n",
    "        if not np.isnan(out) and out < self.r:\n",
    "            self.r = out\n",
    "            self.best_observations = scores\n",
    "        eval_logger.info(f\"Returning {out} as the avg over observed scores {scores}\")\n",
    "        return out\n",
    "    \n",
    "    def lccv90(self, pl, seed): # maximum train size is 90% of the data (like for 10CV)\n",
    "        try:\n",
    "            #enforce_all_anchor_evaluations = self.r == 1\n",
    "            pl = Pipeline(self.mandatory_pre_processing + pl.steps)\n",
    "            args = {\n",
    "                \"r\": self.r,\n",
    "                \"timeout\": self.timeout_per_evaluation,\n",
    "                \"seed\": seed,\n",
    "                \"target_anchor\": .9,\n",
    "                \"min_evals_for_stability\": 3,\n",
    "                \"MAX_EVALUATIONS\": 10,\n",
    "                \"enforce_all_anchor_evaluations\": enforce_all_anchor_evaluations,\n",
    "                \"fix_train_test_folds\": True\n",
    "            }\n",
    "            for key, val in self.other_args.items():\n",
    "                args[key] = val\n",
    "            \n",
    "            score,score_est,elc,model = lccv.lccv(pl, self.X, self.y,r=.90)\n",
    "            self.r = min(self.r, score_est)\n",
    "            return score_est\n",
    "        except KeyboardInterrupt:\n",
    "            raise\n",
    "        except:\n",
    "            eval_logger.info(\"Observed some exception. Returning nan\")\n",
    "            return np.nan\n",
    "\n",
    "    def lccv80(self, pl, seed=None): # maximum train size is 80% of the data (like for 5CV)\n",
    "        try:\n",
    "            enforce_all_anchor_evaluations = self.r == 1\n",
    "            pl = Pipeline(self.mandatory_pre_processing + pl.steps)\n",
    "            args = {\n",
    "                \"r\": self.r,\n",
    "                \"timeout\": self.timeout_per_evaluation,\n",
    "                \"seed\": seed,\n",
    "                \"target_anchor\": .8,\n",
    "                \"min_evals_for_stability\": 3,\n",
    "                \"MAX_EVALUATIONS\": 5,\n",
    "                \"enforce_all_anchor_evaluations\": enforce_all_anchor_evaluations,\n",
    "                \"fix_train_test_folds\": True\n",
    "            }\n",
    "            \n",
    "            for key, val in self.other_args.items():\n",
    "                args[key] = val\n",
    "            print(args)\n",
    "            score,score_est,elc,model = lccv.lccv(pl, self.X, self.y,self.r,enforce_all_anchor_evaluations=True,target_anchor=0.8,seed=seed,min_evals_for_stability=3,fix_train_test_folds=True,timeout=timeout_per_evaluation)\n",
    "            print(pl)\n",
    "            print(score)\n",
    "            self.r = min(self.r, score_est)\n",
    "            return score_est\n",
    "        except KeyboardInterrupt:\n",
    "            raise\n",
    "        except:\n",
    "            eval_logger.info(\"Observed some exception. Returning nan\")\n",
    "            return np.nan\n",
    "\n",
    "    def lccv90flex(self, pl, seed=None): # maximum train size is 90% of the data (like for 10CV)\n",
    "        try:\n",
    "            enforce_all_anchor_evaluations = self.r == 1\n",
    "            pl = Pipeline(self.mandatory_pre_processing + pl.steps)\n",
    "            \n",
    "            args = {\n",
    "                \"r\": self.r,\n",
    "                \"timeout\": self.timeout_per_evaluation,\n",
    "                \"seed\": seed,\n",
    "                \"target_anchor\": .9,\n",
    "                \"min_evals_for_stability\": 3,\n",
    "                \"MAX_EVALUATIONS\": 10,\n",
    "                \"enforce_all_anchor_evaluations\": enforce_all_anchor_evaluations,\n",
    "                \"use_train_curve\": decide_block_train,\n",
    "                \"fix_train_test_folds\": False\n",
    "            }\n",
    "            for key, val in self.other_args.items():\n",
    "                args[key] = val\n",
    "            \n",
    "            _,score,elc,model = lccv.lccv(pl, self.X, self.y,r=0.90)\n",
    "            self.r = min(self.r, score)\n",
    "            return score\n",
    "        except KeyboardInterrupt:\n",
    "            raise\n",
    "        except Exception as e:\n",
    "            traceback.print_exc()\n",
    "            eval_logger.info(\"Observed some exception. Returning nan\")\n",
    "            return np.nan\n",
    "\n",
    "    def lccv80flex(self, pl, seed=None): # maximum train size is 80% of the data (like for 5CV)\n",
    "        try:\n",
    "            #enforce_all_anchor_evaluations = self.r == 1\n",
    "            pl = Pipeline(self.mandatory_pre_processing + pl.steps)\n",
    "            _,score,elc,model = lccv.lccv(pl, self.X, self.y, r=self.r, timeout=self.timeout_per_evaluation, seed=seed, target_anchor=.8, min_evals_for_stability=3, MAX_EVALUATIONS = 5, enforce_all_anchor_evaluations = enforce_all_anchor_evaluations,fix_train_test_folds=False, use_train_curve=decide_block_train, visualize_lcs = False)[0]\n",
    "            self.r = min(self.r, score)\n",
    "            print(score)\n",
    "            return elc\n",
    "        except KeyboardInterrupt:\n",
    "            raise\n",
    "        except Exception as e:\n",
    "            eval_logger.info(f\"Observed some exception. Returning nan. Exception was {e}\")\n",
    "            return np.nan\n",
    "    \n",
    "    def select_model(self, learners, errors = \"ignore\"):\n",
    "        \n",
    "        hard_cutoff = 2 * self.timeout_per_evaluation\n",
    "        r = 1.0\n",
    "        best_score = 1\n",
    "        chosen_learner = None\n",
    "        validation_times = []\n",
    "        exp_logger = logging.getLogger(\"experimenter\")\n",
    "        n = len(learners)\n",
    "        memory_history = []\n",
    "        index_of_best_learner = -1\n",
    "\n",
    "        target_anchor = int(np.floor(self.X.shape[0] * .9))  # TODO hardcoded, please fix\n",
    "        target_anchor_count = 0\n",
    "        learner_crash_count = 0\n",
    "        for i, learner in enumerate(learners):\n",
    "            temp_pipe = self.get_pipeline_from_descriptor(learner)\n",
    "            exp_logger.info(f\"\"\"\n",
    "                --------------------------------------------------\n",
    "                Checking learner {i + 1}/{n} (\"\"\" + str(temp_pipe).replace(\"\\n\", \"\").replace(\"\\t\", \"\").replace(\" \", \"\").replace(\" \", \"\").replace(\" \", \"\").replace(\" \", \"\").replace(\" \", \"\").replace(\" \", \"\").replace(\" \", \"\").replace(\" \", \"\") + \"\"\")\n",
    "                --------------------------------------------------\"\"\")\n",
    "            cur_mem = int(psutil.Process(os.getpid()).memory_info().rss / 1024 / 1024)\n",
    "            memory_history.append(cur_mem)\n",
    "            exp_logger.info(f\"Currently used memory: {cur_mem}MB. Memory history is: {memory_history}\")\n",
    "            \n",
    "            validation_start = time.time()\n",
    "            try:\n",
    "                score = self.validation_func(temp_pipe, seed=13 * self.seed + i)\n",
    "                runtime = time.time() - validation_start\n",
    "                eval_logger.info(f\"Observed score {score} for {format_learner(temp_pipe)}. Validation took {int(np.round(runtime * 1000))}ms\")\n",
    "                r = min(r, score + self.epsilon)\n",
    "                eval_logger.info(f\"r is now: {r}\")\n",
    "                if score < best_score:\n",
    "                    best_score = score\n",
    "                    chosen_learner = temp_pipe\n",
    "                    index_of_best_learner = i\n",
    "                    eval_logger.info(f\"Thas was a NEW BEST score. r has been updated. In other words, currently chosen model is {format_learner(chosen_learner)}\")\n",
    "                else:\n",
    "                    del temp_pipe\n",
    "                    gc.collect()\n",
    "                    eval_logger.info(f\"Candidate was NOT competitive. Eliminating the object and garbage collecting.\")\n",
    "                \n",
    "            except KeyboardInterrupt:\n",
    "                raise\n",
    "            except Exception as e:\n",
    "                del temp_pipe\n",
    "                gc.collect()\n",
    "                exp_logger.info(f\"Candidate was unsuccessful, deleting it from memory.\")\n",
    "                runtime = time.time() - validation_start\n",
    "                \n",
    "                if errors == \"raise\":\n",
    "                    raise e\n",
    "                \n",
    "            validation_times.append(runtime)\n",
    "            \n",
    "        eval_logger.info(f\"Chosen learner was found in iteration {index_of_best_learner + 1}\")\n",
    "        return chosen_learner\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-13-9a3fabf6a814>:36: FutureWarning: Starting from Version 0.15 `download_data`, `download_qualities`, and `download_features_meta_data` will all be ``False`` instead of ``True`` by default to enable lazy loading. To disable this message until version 0.15 explicitly set `download_data`, `download_qualities`, and `download_features_meta_data` to a bool while calling `get_dataset`.\n",
      "  ds = openml.datasets.get_dataset(openmlid)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read in data frame. Size is 1500 x 10001.\n",
      "Data is of shape (1500, 10000).\n",
      "[[ 5.  3.  4. ...  0.  0.  0.]\n",
      " [12.  3.  6. ...  0.  0.  0.]\n",
      " [ 3.  2.  2. ...  0.  1.  0.]\n",
      " ...\n",
      " [ 8. 10.  2. ...  0.  0.  0.]\n",
      " [11. 12. 10. ...  0.  1.  2.]\n",
      " [ 8.  7.  5. ...  0.  1.  1.]] [ 0  0  0 ... 49 49 49]\n"
     ]
    }
   ],
   "source": [
    "X,y=get_dataset(1457)\n",
    "print(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 36s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(nan,\n",
       " 0.22666666666666666,\n",
       " {64: {'n': 3,\n",
       "   'mean': 0.06444444444444444,\n",
       "   'std': 0.00831479419283098,\n",
       "   'conf': array([0.05503554, 0.07385335])},\n",
       "  128: {'n': 3,\n",
       "   'mean': 0.11777777777777781,\n",
       "   'std': 0.00831479419283098,\n",
       "   'conf': array([0.10836888, 0.12718668])},\n",
       "  256: {'n': 5,\n",
       "   'mean': 0.22666666666666666,\n",
       "   'std': 0.015202339001321832,\n",
       "   'conf': array([0.21334147, 0.23999186])},\n",
       "  512: {'n': 4,\n",
       "   'mean': 0.3433333333333333,\n",
       "   'std': 0.017320508075688766,\n",
       "   'conf': array([0.32635955, 0.36030712])},\n",
       "  1024: {'n': 3,\n",
       "   'mean': 0.40444444444444444,\n",
       "   'std': 0.04012326685615066,\n",
       "   'conf': array([0.35904153, 0.44984736])}},\n",
       " <lccv.lccv.EmpiricalLearningModel at 0x1f8601b88b0>)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "lccv.lccv(sklearn.tree.DecisionTreeClassifier(),X,y,r=0.80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 28s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(nan,\n",
       " 0.33916666666666667,\n",
       " {64: {'n': 3,\n",
       "   'mean': 0.060000000000000005,\n",
       "   'std': 0.012472191289246468,\n",
       "   'conf': array([0.04588665, 0.07411335])},\n",
       "  128: {'n': 3,\n",
       "   'mean': 0.14333333333333334,\n",
       "   'std': 0.024944382578492942,\n",
       "   'conf': array([0.11510663, 0.17156004])},\n",
       "  256: {'n': 5,\n",
       "   'mean': 0.2106666666666667,\n",
       "   'std': 0.05242560867023334,\n",
       "   'conf': array([0.16471444, 0.25661889])},\n",
       "  512: {'n': 4,\n",
       "   'mean': 0.33916666666666667,\n",
       "   'std': 0.009537935951882999,\n",
       "   'conf': array([0.32981966, 0.34851367])},\n",
       "  1024: {'n': 3,\n",
       "   'mean': 0.41111111111111115,\n",
       "   'std': 0.017708197167232476,\n",
       "   'conf': array([0.39107277, 0.43114945])}},\n",
       " <lccv.lccv.EmpiricalLearningModel at 0x1f85e64cbe0>)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "lccv.lccv(sklearn.tree.DecisionTreeClassifier(),X,y,r=0.80,target_anchor=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "pipe = Pipeline([('DTC',DecisionTreeClassifier())])\n",
    "selector=VerticalEvaluator(X,y,False,\"cv\",train_size=0.8,timeout_per_evaluation=100,epsilon = 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-13-9a3fabf6a814>:106: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
      "  h1_before, h2_before = hash(X_train.tostring()), hash(X_test.tostring())\n",
      "<ipython-input-13-9a3fabf6a814>:117: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
      "  h1_after, h2_after = hash(X_train.tostring()), hash(X_test.tostring())\n",
      "<ipython-input-13-9a3fabf6a814>:106: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
      "  h1_before, h2_before = hash(X_train.tostring()), hash(X_test.tostring())\n",
      "<ipython-input-13-9a3fabf6a814>:117: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
      "  h1_after, h2_after = hash(X_train.tostring()), hash(X_test.tostring())\n",
      "<ipython-input-13-9a3fabf6a814>:106: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
      "  h1_before, h2_before = hash(X_train.tostring()), hash(X_test.tostring())\n",
      "<ipython-input-13-9a3fabf6a814>:117: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
      "  h1_after, h2_after = hash(X_train.tostring()), hash(X_test.tostring())\n",
      "<ipython-input-13-9a3fabf6a814>:106: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
      "  h1_before, h2_before = hash(X_train.tostring()), hash(X_test.tostring())\n",
      "<ipython-input-13-9a3fabf6a814>:117: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
      "  h1_after, h2_after = hash(X_train.tostring()), hash(X_test.tostring())\n",
      "<ipython-input-13-9a3fabf6a814>:106: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
      "  h1_before, h2_before = hash(X_train.tostring()), hash(X_test.tostring())\n",
      "<ipython-input-13-9a3fabf6a814>:117: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
      "  h1_after, h2_after = hash(X_train.tostring()), hash(X_test.tostring())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 14s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5793333333333333"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "selector.cv(pipe,seed=0,folds=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "selector=VerticalEvaluator(X,y,False,\"wilcoxon\",train_size=0.8,timeout_per_evaluation=100,epsilon = 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-13-9a3fabf6a814>:106: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
      "  h1_before, h2_before = hash(X_train.tostring()), hash(X_test.tostring())\n",
      "<ipython-input-13-9a3fabf6a814>:117: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
      "  h1_after, h2_after = hash(X_train.tostring()), hash(X_test.tostring())\n",
      "<ipython-input-13-9a3fabf6a814>:106: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
      "  h1_before, h2_before = hash(X_train.tostring()), hash(X_test.tostring())\n",
      "<ipython-input-13-9a3fabf6a814>:117: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
      "  h1_after, h2_after = hash(X_train.tostring()), hash(X_test.tostring())\n",
      "<ipython-input-13-9a3fabf6a814>:106: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
      "  h1_before, h2_before = hash(X_train.tostring()), hash(X_test.tostring())\n",
      "<ipython-input-13-9a3fabf6a814>:117: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
      "  h1_after, h2_after = hash(X_train.tostring()), hash(X_test.tostring())\n",
      "<ipython-input-13-9a3fabf6a814>:106: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
      "  h1_before, h2_before = hash(X_train.tostring()), hash(X_test.tostring())\n",
      "<ipython-input-13-9a3fabf6a814>:117: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
      "  h1_after, h2_after = hash(X_train.tostring()), hash(X_test.tostring())\n",
      "<ipython-input-13-9a3fabf6a814>:106: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
      "  h1_before, h2_before = hash(X_train.tostring()), hash(X_test.tostring())\n",
      "<ipython-input-13-9a3fabf6a814>:117: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
      "  h1_after, h2_after = hash(X_train.tostring()), hash(X_test.tostring())\n",
      "<ipython-input-13-9a3fabf6a814>:106: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
      "  h1_before, h2_before = hash(X_train.tostring()), hash(X_test.tostring())\n",
      "<ipython-input-13-9a3fabf6a814>:117: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
      "  h1_after, h2_after = hash(X_train.tostring()), hash(X_test.tostring())\n",
      "<ipython-input-13-9a3fabf6a814>:106: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
      "  h1_before, h2_before = hash(X_train.tostring()), hash(X_test.tostring())\n",
      "<ipython-input-13-9a3fabf6a814>:106: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
      "  h1_before, h2_before = hash(X_train.tostring()), hash(X_test.tostring())\n",
      "<ipython-input-13-9a3fabf6a814>:106: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
      "  h1_before, h2_before = hash(X_train.tostring()), hash(X_test.tostring())\n",
      "<ipython-input-13-9a3fabf6a814>:106: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
      "  h1_before, h2_before = hash(X_train.tostring()), hash(X_test.tostring())\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5522222222222222"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selector.wilcoxon(pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "selector=SH(X,y,False,100,64)\n",
    "pipe = Pipeline(steps=[(\"dataprocessor\",MultinomialNB())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b_max is 96000\n",
      "Eta is 0.0006666666666666666\n",
      "There will be 0 phases with the following setup.\n"
     ]
    }
   ],
   "source": [
    "selector.select_model(pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.ensemble\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import sklearn.naive_bayes\n",
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class SH in module __main__:\n",
      "\n",
      "class SH(Evaluator)\n",
      " |  SH(X, y, binarize_sparse, timeout_per_evaluation, max_train_budget, b_min=64, seed=0, repeats=10)\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      SH\n",
      " |      Evaluator\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, X, y, binarize_sparse, timeout_per_evaluation, max_train_budget, b_min=64, seed=0, repeats=10)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  select_model(self, learners)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from Evaluator:\n",
      " |  \n",
      " |  eval_pipeline_on_fold(self, pl, X_train, X_test, y_train, y_test, timeout=None)\n",
      " |  \n",
      " |  get_pipeline_from_descriptor(self, learner)\n",
      " |  \n",
      " |  get_result_of_cv(self, learner_inst, folds, seed=None, timeout=None)\n",
      " |  \n",
      " |  mccv(self, learner, target_size=0.9, timeout=None, seed=0, repeats=10)\n",
      " |      Conducts a 90/10 MCCV (imitating a bit a 10-fold cross validation)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from Evaluator:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(SH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class VerticalEvaluator in module __main__:\n",
      "\n",
      "class VerticalEvaluator(Evaluator)\n",
      " |  VerticalEvaluator(X, y, binarize_sparse, validation, train_size, timeout_per_evaluation, epsilon, seed=0, exception_on_failure=False, other_args={}, best_observations=None)\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      VerticalEvaluator\n",
      " |      Evaluator\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, X, y, binarize_sparse, validation, train_size, timeout_per_evaluation, epsilon, seed=0, exception_on_failure=False, other_args={}, best_observations=None)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  cv(self, pl, seed, folds)\n",
      " |  \n",
      " |  lccv80(self, pl, seed=None)\n",
      " |  \n",
      " |  lccv80flex(self, pl, seed=None)\n",
      " |  \n",
      " |  lccv90(self, pl, seed)\n",
      " |  \n",
      " |  lccv90flex(self, pl, seed=None)\n",
      " |  \n",
      " |  select_model(self, learners, errors='ignore')\n",
      " |  \n",
      " |  wilcoxon(self, pl, seed=0, folds=10)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from Evaluator:\n",
      " |  \n",
      " |  eval_pipeline_on_fold(self, pl, X_train, X_test, y_train, y_test, timeout=None)\n",
      " |  \n",
      " |  get_pipeline_from_descriptor(self, learner)\n",
      " |  \n",
      " |  get_result_of_cv(self, learner_inst, folds, seed=None, timeout=None)\n",
      " |  \n",
      " |  mccv(self, learner, target_size=0.9, timeout=None, seed=0, repeats=10)\n",
      " |      Conducts a 90/10 MCCV (imitating a bit a 10-fold cross validation)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from Evaluator:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(VerticalEvaluator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function lccv in module lccv.lccv:\n",
      "\n",
      "lccv(learner_inst, X, y, r, timeout=None, base=2, min_exp=6, MAX_ESTIMATE_MARGIN_FOR_FULL_EVALUATION=0.005, MAX_EVALUATIONS=10, target_anchor=0.9, schedule=None, return_estimate_on_incomplete_runs=False, max_conf_interval_size_default=0.1, max_conf_interval_size_target=0.001, enforce_all_anchor_evaluations=False, seed=0, verbose=False, logger=None, min_evals_for_stability=3, use_train_curve=True, fix_train_test_folds=False, evaluator=None, scoring='accuracy', visualize_lcs=False, exceptions='message')\n",
      "    Evaluates a learner in an iterative fashion, using learning curves. The\n",
      "    method builds upon the assumption that learning curves are convex. After\n",
      "    each iteration, it checks whether the convexity assumption is still valid.\n",
      "    If not, it tries to repair it.\n",
      "    Also, after each iteration it checks whether the performance of the best\n",
      "    seen learner so far is still reachable by making an optimistic extrapolation.\n",
      "    If not, it stops the evaluation.\n",
      "    \n",
      "    :param learner_inst: The learner to be evaluated\n",
      "    :param X: The features on which the learner needs to be evaluated\n",
      "    :param y: The labels on which the learner needs to be trained\n",
      "    :param r: The best seen performance so far (lower is better). Fill in 0.0 if\n",
      "    no learners have been evaluated prior to the learner.\n",
      "    :param timeout: The maximal runtime for this specific leaner. Fill in None\n",
      "    to avoid cutting of the evaluation.\n",
      "    :param base: The base factor to increase the sample sizes of the learning\n",
      "    curve.\n",
      "    :param min_exp: The first exponent of the learning curve.\n",
      "    :param MAX_ESTIMATE_MARGIN_FOR_FULL_EVALUATION: The maximum number of\n",
      "    evaluations to be performed\n",
      "    :param MAX_EVALUATIONS:\n",
      "    :param target_anchor:\n",
      "    :param schedule: define the anchors for which scores should be computed\n",
      "    :param return_estimate_on_incomplete_runs:\n",
      "    :param max_conf_interval_size_default:\n",
      "    :param max_conf_interval_size_target:\n",
      "    :param enforce_all_anchor_evaluations:\n",
      "    :param seed:\n",
      "    :param verbose:\n",
      "    :param logger:\n",
      "    :param min_evals_for_stability:\n",
      "    :param use_train_curve: If True, then the evaluation stops as soon as the train curve drops under the threshold r\n",
      "    :param evaluator: Function to be used to query a noisy score at some anchor. To be maximized!\n",
      "    :param scoring: Scoring function to be computed for predictions obtained at an anchor. Is ignored if an evaluator is given.\n",
      "    :return:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(lccv.lccv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
