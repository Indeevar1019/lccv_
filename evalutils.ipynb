{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import openml\n",
    "import lccv\n",
    "import os, psutil\n",
    "import gc\n",
    "import logging\n",
    "import traceback\n",
    "\n",
    "from func_timeout import func_timeout, FunctionTimedOut\n",
    "\n",
    "import time\n",
    "import random\n",
    "\n",
    "import itertools as it\n",
    "import scipy.stats\n",
    "from scipy.sparse import lil_matrix\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "import sklearn\n",
    "from sklearn import metrics\n",
    "from sklearn import *\n",
    "\n",
    "from func_timeout import func_timeout, FunctionTimedOut\n",
    "#from commons import *\n",
    "from tqdm import tqdm\n",
    "\n",
    "import import_ipynb\n",
    "import Commons\n",
    "\n",
    "eval_logger = logging.getLogger(\"evalutils\")\n",
    "\n",
    "\n",
    "def get_dataset(openmlid):\n",
    "    ds = openml.datasets.get_dataset(openmlid)\n",
    "    df = ds.get_data()[0]\n",
    "    num_rows = len(df)\n",
    "        \n",
    "    # prepare label column as numpy array\n",
    "    print(f\"Read in data frame. Size is {len(df)} x {len(df.columns)}.\")\n",
    "    X = np.array(df.drop(columns=[ds.default_target_attribute]).values)\n",
    "    y = np.array(df[ds.default_target_attribute].values)\n",
    "    if y.dtype != int:\n",
    "        y_int = np.zeros(len(y)).astype(int)\n",
    "        vals = np.unique(y)\n",
    "        for i, val in enumerate(vals):\n",
    "            mask = y == val\n",
    "            y_int[mask] = i\n",
    "        y = y_int\n",
    "        \n",
    "    print(f\"Data is of shape {X.shape}.\")\n",
    "    return X, y\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def format_learner(learner):\n",
    "    learner_name = str(learner).replace(\"\\n\", \" \").replace(\"\\t\", \" \")\n",
    "    for k in  range(20):\n",
    "        learner_name = learner_name.replace(\"  \", \" \")\n",
    "    return learner_name\n",
    "\n",
    "\n",
    "def decide_block_train(pl, anchor):\n",
    "    steps = pl.steps\n",
    "    predictor = steps[-1][1]\n",
    "    if type(predictor) == sklearn.ensemble.HistGradientBoostingClassifier:\n",
    "        min_samples_leaf = predictor.min_samples_leaf\n",
    "        return anchor >= 2* min_samples_leaf\n",
    "    \n",
    "    return True\n",
    "    \n",
    "class Evaluator:\n",
    "    \n",
    "    def __init__(self, X, y, binarize_sparse = False):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.best_observations=None\n",
    "        # determine fixed pre-processing steps for imputation and binarization\n",
    "        types = [set([type(v) for v in r]) for r in X.T]\n",
    "        numeric_features = [c for c, t in enumerate(types) if len(t) == 1 and list(t)[0] != str]\n",
    "        numeric_transformer = Pipeline([(\"imputer\", sklearn.impute.SimpleImputer(strategy=\"median\"))])\n",
    "        categorical_features = [i for i in range(X.shape[1]) if i not in numeric_features]\n",
    "        missing_values_per_feature = np.sum(pd.isnull(X), axis=0)\n",
    "        eval_logger.info(f\"There are {len(categorical_features)} categorical features, which will be binarized.\")\n",
    "        eval_logger.info(f\"Missing values for the different attributes are {missing_values_per_feature}.\")\n",
    "        if len(categorical_features) > 0 or sum(missing_values_per_feature) > 0:\n",
    "            categorical_transformer = Pipeline([\n",
    "                (\"imputer\", sklearn.impute.SimpleImputer(strategy=\"most_frequent\")),\n",
    "                (\"binarizer\", sklearn.preprocessing.OneHotEncoder(handle_unknown='ignore', sparse = binarize_sparse)),\n",
    "            ])\n",
    "            self.mandatory_pre_processing = [(\"impute_and_binarize\", ColumnTransformer(\n",
    "                transformers=[\n",
    "                    (\"num\", numeric_transformer, numeric_features),\n",
    "                    (\"cat\", categorical_transformer, categorical_features),\n",
    "                ]\n",
    "            ))]\n",
    "        else:\n",
    "            self.mandatory_pre_processing = []\n",
    "    \n",
    "    def eval_pipeline_on_fold(self, pl, X_train, X_test, y_train, y_test, timeout = None):\n",
    "        try:\n",
    "            pl = Pipeline(self.mandatory_pre_processing + sklearn.base.clone(pl).steps)\n",
    "            \n",
    "            h1_before, h2_before = hash(X_train.tostring()), hash(X_test.tostring())\n",
    "            if timeout is None:\n",
    "                eval_logger.info(f\"Fitting model with {X_train.shape[0]} instances and without timeout.\")\n",
    "                pl.fit(X_train, y_train)\n",
    "            else:\n",
    "                eval_logger.info(f\"Fitting model with {X_train.shape[0]} instances and timeout {timeout}.\")\n",
    "                func_timeout(timeout, pl.fit, (X_train, y_train))\n",
    "                \n",
    "            y_hat = pl.predict(X_test)\n",
    "            error_rate = 1 - sklearn.metrics.accuracy_score(y_test, y_hat)\n",
    "            eval_logger.info(f\"Observed an error rate of {error_rate}\")\n",
    "            h1_after, h2_after = hash(X_train.tostring()), hash(X_test.tostring())\n",
    "            if h1_before != h1_after or h2_before != h2_after:\n",
    "                raise Exception(\"Pipeline has modified the original data, which is forbidden!\")\n",
    "            return error_rate\n",
    "        \n",
    "        except FunctionTimedOut:\n",
    "            eval_logger.info(f\"Timeout observed for evaluation, stopping and returning nan.\")\n",
    "        except KeyboardInterrupt:\n",
    "            raise\n",
    "        except Exception as e:\n",
    "            eval_logger.info(f\"Observed some exception. Stopping. Exception: {e}\")\n",
    "        \n",
    "        return np.nan\n",
    "    \n",
    "    def mccv(self, learner, target_size=.9, timeout=None, seed=0, repeats = 10):\n",
    "\n",
    "        \"\"\"\n",
    "        Conducts a 90/10 MCCV (imitating a bit a 10-fold cross validation)\n",
    "        \"\"\"\n",
    "        eval_logger.info(f\"Running mccv with seed  {seed}\")\n",
    "        if not timeout is None:\n",
    "            deadline = time.time() + timeout\n",
    "\n",
    "        scores = []\n",
    "        n = self.X.shape[0]\n",
    "        num_examples = int(target_size * n)\n",
    "        deadline = None if timeout is None else time.time() + timeout\n",
    "\n",
    "        seed *= 13\n",
    "        for r in range(repeats):\n",
    "            eval_logger.info(f\"Seed in MCCV: {seed}. Training on {num_examples} examples. That is {np.round(100 * num_examples / self.X.shape[0])}% of the data (testing on rest).\")\n",
    "            \n",
    "            # get random train/test split based on seed\n",
    "            random.seed(seed)\n",
    "            n = self.X.shape[0]\n",
    "            indices_train = random.sample(range(n), num_examples)\n",
    "            mask_train = np.zeros(n)\n",
    "            mask_train[indices_train] = 1\n",
    "            mask_train = mask_train.astype(bool)\n",
    "            mask_test = (1 - mask_train).astype(bool)\n",
    "            X_train = self.X[mask_train]\n",
    "            y_train = self.y[mask_train]\n",
    "            X_test = self.X[mask_test]\n",
    "            y_test = self.y[mask_test]\n",
    "            \n",
    "            # evaluate pipeline\n",
    "            timeout_local = None if timeout is None else deadline - time.time()\n",
    "            error_rate = self.eval_pipeline_on_fold(learner, X_train, X_test, y_train, y_test, timeout=timeout_local)\n",
    "            scores.append(error_rate)\n",
    "            seed += 1\n",
    "            del X_train, X_test\n",
    "        gc.collect()\n",
    "\n",
    "        return scores\n",
    "    \n",
    "    def get_result_of_cv(self, folds, seed = None, timeout = None):\n",
    "        kf = sklearn.model_selection.KFold(n_splits=folds, random_state=np.random.RandomState(seed), shuffle=True)\n",
    "        scores = []\n",
    "        deadline = time.time() + timeout if timeout is not None else None\n",
    "        for train_index, test_index in kf.split(X):\n",
    "            X_train, y_train = X[train_index], y[train_index]\n",
    "            X_test, y_test = X[test_index], y[test_index]\n",
    "            timeout_loc = None if timeout is None else deadline - time.time()\n",
    "            error_rate = eval_pipeline_on_fold(learner_inst, X_train, X_test, y_train, y_test, timeout = timeout_loc)\n",
    "            if not np.isnan(error_rate):\n",
    "                scores.append(error_rate)\n",
    "        out = np.mean(scores) if scores else np.nan\n",
    "        eval_logger.info(f\"Returning {out} as the avg over observed scores {scores}\")\n",
    "        return out\n",
    "    \n",
    "\n",
    "    def get_pipeline_from_descriptor(self, learner):\n",
    "        return sklearn.pipeline.Pipeline([(step_name, build_estimator(comp, params, self.X, self.y)) for step_name, (comp, params) in learner])\n",
    "\n",
    "    '''\n",
    "        This is the main function that must be implemented by the approaches\n",
    "    '''\n",
    "    def select_model(self, learners):\n",
    "        raise NotImplemented()\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "class SH(Evaluator):\n",
    "    \n",
    "    def __init__(self, X, y, binarize_sparse, timeout_per_evaluation, max_train_budget, b_min = 64, seed = 0, repeats = 10):\n",
    "        self.timeout_per_evaluation = timeout_per_evaluation\n",
    "        self.b_min = b_min\n",
    "        self.seed = seed\n",
    "        self.repeats = repeats\n",
    "        self.max_train_budget = max_train_budget\n",
    "        self.r=0\n",
    "        super().__init__(X, y, binarize_sparse)\n",
    "    \n",
    "    def select_model(self, learners):\n",
    "        b_min = self.b_min\n",
    "        test_budget = 1 - self.max_train_budget\n",
    "        b_max = int(self.X.shape[0] * (1 - test_budget))\n",
    "        timeout = self.timeout_per_evaluation\n",
    "        print(f\"b_max is {b_max}\")\n",
    "        n = len(learners)\n",
    "        num_phases = int(np.log2(n) - 1)\n",
    "        eta = (b_max / b_min)**(1/num_phases)\n",
    "        print(f\"Eta is {eta}\")\n",
    "        anchors = [int(np.round(b_min * eta**i)) for i in range(num_phases + 1)]\n",
    "        populations = [int(np.round(n / (2**i))) for i in range(num_phases + 1)]\n",
    "        if num_phases != int(num_phases):\n",
    "            raise Exception(f\"Number of learners is {len(learners)}, which is not a power of 2!\")\n",
    "        num_phases = int(num_phases)\n",
    "        print(f\"There will be {num_phases + 1} phases with the following setup.\")\n",
    "        for anchor, population in zip(anchors, populations):\n",
    "            print(f\"Evaluate {population} on {anchor}\")\n",
    "\n",
    "        best_seen_score = np.inf\n",
    "        best_seen_pl = None\n",
    "\n",
    "        def get_scores_on_budget(candidates, budget):\n",
    "            scores = []\n",
    "            for candidate in tqdm(candidates):\n",
    "                deadline = None if timeout is None else time.time() + timeout\n",
    "\n",
    "                temp_pipe = self.get_pipeline_from_descriptor(candidate)\n",
    "                scores_for_candidate_at_budget = []\n",
    "                for i in range(self.repeats):\n",
    "                    if deadline < time.time():\n",
    "                        break\n",
    "                    try:\n",
    "                        X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(self.X, self.y, train_size = budget, test_size = test_budget)\n",
    "                        error_rate = self.eval_pipeline_on_fold(temp_pipe, X_train, X_test, y_train, y_test, deadline - time.time())\n",
    "                        if not np.isnan(error_rate):\n",
    "                            scores_for_candidate_at_budget.append(np.round(error_rate, 4))\n",
    "                        else:\n",
    "                            scores_for_candidate_at_budget.append(np.nan)\n",
    "                    except KeyboardInterrupt:\n",
    "                        raise\n",
    "                    except Exception as e:\n",
    "                        print(f\"There was an error in the evaluation of candidate {candidate}. Ignoring it. Error: {e}\")\n",
    "                        scores_for_candidate_at_budget.append(np.nan)\n",
    "                \n",
    "                scores.append(scores_for_candidate_at_budget)\n",
    "                \n",
    "            return scores\n",
    "\n",
    "        time_start = time.time()\n",
    "        population = learners.copy()\n",
    "        for i, anchor in enumerate(anchors):\n",
    "            time_start_phase = time.time()\n",
    "            scores_in_round = get_scores_on_budget(population, anchor)\n",
    "            runtime_phase = time.time() - time_start_phase\n",
    "            mean_scores_tmp = [np.nanmean(s) if np.count_nonzero(np.isnan(s)) < len(s) else np.nan for s in scores_in_round]\n",
    "            if all(np.isnan(mean_scores_tmp)):\n",
    "                print(\"All candidates evalated nan in last round, aborting evaluation.\")\n",
    "                break\n",
    "            mean_scores = mean_scores_tmp\n",
    "            index_of_best_mean_score_in_round = np.nanargmin(mean_scores)\n",
    "            best_mean_score_in_round = mean_scores[index_of_best_mean_score_in_round]\n",
    "            if best_mean_score_in_round < best_seen_score:\n",
    "                best_seen_score = best_mean_score_in_round\n",
    "                best_seen_pl = population[index_of_best_mean_score_in_round]\n",
    "\n",
    "            print(f\"Finished round {i+1} after {np.round(runtime_phase, 2)}s. Scores are: {mean_scores}.\\nBest score was: {best_mean_score_in_round} (all times best score was {best_seen_score})\")\n",
    "            best_indices = np.argsort(mean_scores)[:int(len(population) / 2)]\n",
    "            print(f\"Best indices are: {best_indices}.\")\n",
    "            if len(population) > 2:\n",
    "                population = [p for j, p in enumerate(population) if j in best_indices]\n",
    "        runtime = time.time () - time_start\n",
    "\n",
    "        return self.get_pipeline_from_descriptor(best_seen_pl)\n",
    "    \n",
    "\n",
    "class VerticalEvaluator(Evaluator):\n",
    "    \n",
    "    def __init__(self, X, y, binarize_sparse, validation, train_size, timeout_per_evaluation, epsilon, seed=0, exception_on_failure=False, other_args = {},best_observations=None):\n",
    "        super().__init__(X, y, binarize_sparse)\n",
    "        self.r=0\n",
    "        self.best_observations = best_observations\n",
    "        self.other_args = other_args\n",
    "        if validation == \"cv\":\n",
    "            if train_size == 0.8:\n",
    "                num_folds = 5\n",
    "            elif train_size == 0.9:\n",
    "                num_folds = 10\n",
    "            else:\n",
    "                raise ValueError(f\"Cannot run cross-validation for train_size {train_size}. Must be 0.8 or 0.9.\")\n",
    "            self.validation_func = lambda pl, seed: self.cv(pl, seed, num_folds, *self.other_args)\n",
    "        elif \"lccv\" in validation:\n",
    "            \n",
    "            is_flex = \"flex\" in validation\n",
    "            \n",
    "            self.r = 1.0\n",
    "            if train_size == 0.8:\n",
    "                self.validation_func = self.lccv80flex if is_flex else self.lccv80\n",
    "            elif train_size == 0.9:\n",
    "                self.validation_func = self.lccv90flex if is_flex else self.lccv90\n",
    "            else:\n",
    "                raise ValueError(f\"Cannot run LCCV for train_size {train_size}. Must be 0.8 or 0.9.\")\n",
    "        elif validation == \"wilcoxon\":\n",
    "            self.r = 1.0\n",
    "            self.best_observations = None\n",
    "            if train_size == 0.8:\n",
    "                self.validation_func = lambda pl, seed: self.wilcoxon(pl, seed = seed, folds = 5)\n",
    "            elif train_size == 0.9:\n",
    "                self.validation_func = lambda pl, seed: self.wilcoxon(pl, seed = seed, folds = 10)\n",
    "            else:\n",
    "                raise ValueError(f\"Cannot run Wilcoxon for train_size {train_size}. Must be 0.8 or 0.9.\")\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported validation function {validation}.\")\n",
    "        self.timeout_per_evaluation = timeout_per_evaluation\n",
    "        self.epsilon = epsilon\n",
    "        self.seed = seed\n",
    "        self.exception_on_failure = exception_on_failure\n",
    "        \n",
    "    def cv(self, pl, seed, folds):\n",
    "        kf = sklearn.model_selection.KFold(n_splits=folds, random_state=np.random.RandomState(seed), shuffle=True)\n",
    "        scores = []\n",
    "        deadline = time.time() + self.timeout_per_evaluation if self.timeout_per_evaluation is not None else None\n",
    "        for train_index, test_index in kf.split(self.X):\n",
    "            learner_inst_copy = sklearn.base.clone(pl)\n",
    "            X_train, y_train = self.X[train_index], self.y[train_index]\n",
    "            X_test, y_test = self.X[test_index], self.y[test_index]\n",
    "            timeout_loc = None if deadline is None else deadline - time.time()\n",
    "            scores.append(self.eval_pipeline_on_fold(pl, X_train, X_test, y_train, y_test, timeout = timeout_loc))\n",
    "        require_at_least_two = time.time() < deadline\n",
    "        is_valid_result = len(scores) > 0 and ((not require_at_least_two) or np.count_nonzero(np.isnan(scores)) < folds - 1)\n",
    "        out = np.nanmean(scores) if is_valid_result else np.nan # require at least two valid samples in the batch if the timeout was not hit\n",
    "        eval_logger.info(f\"Returning {out} as the avg over observed scores {scores}\")\n",
    "        return out\n",
    "    \n",
    "    def wilcoxon(self, pl, seed=0, folds = 10):\n",
    "\n",
    "        eval_logger.info(f\"Running Wilcoxon-guarded CV with seed  {seed}\")\n",
    "        if not self.timeout_per_evaluation is None:\n",
    "            deadline = time.time() + self.timeout_per_evaluation\n",
    "        \n",
    "        kf = sklearn.model_selection.KFold(n_splits=folds, random_state=np.random.RandomState(seed), shuffle=True)\n",
    "        scores = []\n",
    "        deadline = time.time() + self.timeout_per_evaluation if self.timeout_per_evaluation is not None else None\n",
    "        for inner_run, (train_index, test_index) in enumerate(kf.split(self.X)):\n",
    "            learner_inst_copy = sklearn.base.clone(pl)\n",
    "            X_train, y_train = self.X[train_index], self.y[train_index]\n",
    "            X_test, y_test = self.X[test_index], self.y[test_index]\n",
    "            timeout_loc = None if deadline is None else deadline - time.time()\n",
    "            scores.append(self.eval_pipeline_on_fold(pl, X_train, X_test, y_train, y_test, timeout = timeout_loc))\n",
    "\n",
    "            # now conduct a wilcoxon signed rank test to determine whether significance has been reached\n",
    "            scores_currently_best = np.array(self.best_observations[:len(scores)]) if self.best_observations is not None else np.ones(len(scores))\n",
    "            eval_logger.info(f\"Currently best observations after {inner_run + 1} evaluations: {np.round(scores_currently_best, 2)}\")\n",
    "            eval_logger.info(f\"Current cand.  observations after {inner_run + 1} evaluations: {np.round(scores, 2)}\")\n",
    "            if any(np.array(scores) != scores_currently_best):\n",
    "                statistic, pval = scipy.stats.wilcoxon(scores, scores_currently_best)\n",
    "                eval_logger.info(f\"p-value is {pval}\")\n",
    "                if pval < 0.05:\n",
    "                    eval_logger.info(f\"reached certainty in fold {inner_run + 1}.\")\n",
    "                    if np.mean(scores) > self.r:\n",
    "                        eval_logger.info(\"it is certainly worse, so aborting.\")\n",
    "                        break\n",
    "                    else:\n",
    "                        eval_logger.info(\"it is certainly better, so continuing\")\n",
    "            else:\n",
    "                eval_logger.info(\"omitting test, because all scores are still identical\")\n",
    "        require_at_least_two = time.time() < deadline\n",
    "        is_valid_result = len(scores) > 0 and ((not require_at_least_two) or np.count_nonzero(np.isnan(scores)) < folds - 1)\n",
    "        out = np.nanmean(scores) if is_valid_result else np.nan # require at least two valid samples in the batch if the timeout was not hit\n",
    "        if not np.isnan(out) and out < self.r:\n",
    "            self.r = out\n",
    "            self.best_observations = scores\n",
    "        eval_logger.info(f\"Returning {out} as the avg over observed scores {scores}\")\n",
    "        return out\n",
    "    \n",
    "    def lccv90(self, pl, seed): # maximum train size is 90% of the data (like for 10CV)\n",
    "        try:\n",
    "            #enforce_all_anchor_evaluations = self.r == 1\n",
    "            pl = Pipeline(self.mandatory_pre_processing + pl.steps)\n",
    "            args = {\n",
    "                \"r\": self.r,\n",
    "                \"timeout\": self.timeout_per_evaluation,\n",
    "                \"seed\": seed,\n",
    "                \"target_anchor\": .9,\n",
    "                \"min_evals_for_stability\": 3,\n",
    "                \"MAX_EVALUATIONS\": 10,\n",
    "                \"enforce_all_anchor_evaluations\": enforce_all_anchor_evaluations,\n",
    "                \"fix_train_test_folds\": True\n",
    "            }\n",
    "            for key, val in self.other_args.items():\n",
    "                args[key] = val\n",
    "            \n",
    "            score = lccv.lccv(pl, self.X, self.y, **args)[0]\n",
    "            self.r = min(self.r, score)\n",
    "            return score\n",
    "        except KeyboardInterrupt:\n",
    "            raise\n",
    "        except:\n",
    "            eval_logger.info(\"Observed some exception. Returning nan\")\n",
    "            return np.nan\n",
    "\n",
    "    def lccv80(self, pl, seed=None): # maximum train size is 80% of the data (like for 5CV)\n",
    "        try:\n",
    "            enforce_all_anchor_evaluations = self.r == 1\n",
    "            pl = Pipeline(self.mandatory_pre_processing + pl.steps)\n",
    "            args = {\n",
    "                \"r\": self.r,\n",
    "                \"timeout\": self.timeout_per_evaluation,\n",
    "                \"seed\": seed,\n",
    "                \"target_anchor\": .8,\n",
    "                \"min_evals_for_stability\": 3,\n",
    "                \"MAX_EVALUATIONS\": 5,\n",
    "                \"enforce_all_anchor_evaluations\": enforce_all_anchor_evaluations,\n",
    "                \"fix_train_test_folds\": True\n",
    "            }\n",
    "            for key, val in self.other_args.items():\n",
    "                args[key] = val\n",
    "            score = lccv.lccv(pl, self.X, self.y, **args)[0]\n",
    "            self.r = min(self.r, score)\n",
    "            return score\n",
    "        except KeyboardInterrupt:\n",
    "            raise\n",
    "        except:\n",
    "            eval_logger.info(\"Observed some exception. Returning nan\")\n",
    "            return np.nan\n",
    "\n",
    "    def lccv90flex(self, pl, seed=None): # maximum train size is 90% of the data (like for 10CV)\n",
    "        try:\n",
    "            enforce_all_anchor_evaluations = self.r == 1\n",
    "            pl = Pipeline(self.mandatory_pre_processing + pl.steps)\n",
    "            \n",
    "            args = {\n",
    "                \"r\": self.r,\n",
    "                \"timeout\": self.timeout_per_evaluation,\n",
    "                \"seed\": seed,\n",
    "                \"target_anchor\": .9,\n",
    "                \"min_evals_for_stability\": 3,\n",
    "                \"MAX_EVALUATIONS\": 10,\n",
    "                \"enforce_all_anchor_evaluations\": enforce_all_anchor_evaluations,\n",
    "                \"use_train_curve\": decide_block_train,\n",
    "                \"fix_train_test_folds\": False\n",
    "            }\n",
    "            for key, val in self.other_args.items():\n",
    "                args[key] = val\n",
    "            \n",
    "            score = lccv.lccv(pl, self.X, self.y, **args)[0]\n",
    "            self.r = min(self.r, score)\n",
    "            return score\n",
    "        except KeyboardInterrupt:\n",
    "            raise\n",
    "        except Exception as e:\n",
    "            traceback.print_exc()\n",
    "            eval_logger.info(\"Observed some exception. Returning nan\")\n",
    "            return np.nan\n",
    "\n",
    "    def lccv80flex(self, pl, seed=None): # maximum train size is 80% of the data (like for 5CV)\n",
    "        try:\n",
    "            #enforce_all_anchor_evaluations = self.r == 1\n",
    "            pl = Pipeline(self.mandatory_pre_processing + pl.steps)\n",
    "            score = lccv.lccv(pl, self.X, self.y, r=self.r, timeout=self.timeout_per_evaluation, seed=seed, target_anchor=.8, min_evals_for_stability=3, MAX_EVALUATIONS = 5, enforce_all_anchor_evaluations = enforce_all_anchor_evaluations,fix_train_test_folds=False, use_train_curve=decide_block_train, visualize_lcs = False)[0]\n",
    "            self.r = min(self.r, score)\n",
    "            return score\n",
    "        except KeyboardInterrupt:\n",
    "            raise\n",
    "        except Exception as e:\n",
    "            eval_logger.info(f\"Observed some exception. Returning nan. Exception was {e}\")\n",
    "            return np.nan\n",
    "    \n",
    "    def select_model(self, learners, errors = \"ignore\"):\n",
    "        \n",
    "        hard_cutoff = 2 * self.timeout_per_evaluation\n",
    "        r = 1.0\n",
    "        best_score = 1\n",
    "        chosen_learner = None\n",
    "        validation_times = []\n",
    "        exp_logger = logging.getLogger(\"experimenter\")\n",
    "        n = len(learners)\n",
    "        memory_history = []\n",
    "        index_of_best_learner = -1\n",
    "\n",
    "        target_anchor = int(np.floor(self.X.shape[0] * .9))  # TODO hardcoded, please fix\n",
    "        target_anchor_count = 0\n",
    "        learner_crash_count = 0\n",
    "        for i, learner in enumerate(learners):\n",
    "            temp_pipe = self.get_pipeline_from_descriptor(learner)\n",
    "            exp_logger.info(f\"\"\"\n",
    "                --------------------------------------------------\n",
    "                Checking learner {i + 1}/{n} (\"\"\" + str(temp_pipe).replace(\"\\n\", \"\").replace(\"\\t\", \"\").replace(\" \", \"\").replace(\" \", \"\").replace(\" \", \"\").replace(\" \", \"\").replace(\" \", \"\").replace(\" \", \"\").replace(\" \", \"\").replace(\" \", \"\") + \"\"\")\n",
    "                --------------------------------------------------\"\"\")\n",
    "            cur_mem = int(psutil.Process(os.getpid()).memory_info().rss / 1024 / 1024)\n",
    "            memory_history.append(cur_mem)\n",
    "            exp_logger.info(f\"Currently used memory: {cur_mem}MB. Memory history is: {memory_history}\")\n",
    "            \n",
    "            validation_start = time.time()\n",
    "            try:\n",
    "                score = self.validation_func(temp_pipe, seed=13 * self.seed + i)\n",
    "                runtime = time.time() - validation_start\n",
    "                eval_logger.info(f\"Observed score {score} for {format_learner(temp_pipe)}. Validation took {int(np.round(runtime * 1000))}ms\")\n",
    "                r = min(r, score + self.epsilon)\n",
    "                eval_logger.info(f\"r is now: {r}\")\n",
    "                if score < best_score:\n",
    "                    best_score = score\n",
    "                    chosen_learner = temp_pipe\n",
    "                    index_of_best_learner = i\n",
    "                    eval_logger.info(f\"Thas was a NEW BEST score. r has been updated. In other words, currently chosen model is {format_learner(chosen_learner)}\")\n",
    "                else:\n",
    "                    del temp_pipe\n",
    "                    gc.collect()\n",
    "                    eval_logger.info(f\"Candidate was NOT competitive. Eliminating the object and garbage collecting.\")\n",
    "                \n",
    "            except KeyboardInterrupt:\n",
    "                raise\n",
    "            except Exception as e:\n",
    "                del temp_pipe\n",
    "                gc.collect()\n",
    "                exp_logger.info(f\"Candidate was unsuccessful, deleting it from memory.\")\n",
    "                runtime = time.time() - validation_start\n",
    "                \n",
    "                if errors == \"raise\":\n",
    "                    raise e\n",
    "                \n",
    "            validation_times.append(runtime)\n",
    "            \n",
    "        eval_logger.info(f\"Chosen learner was found in iteration {index_of_best_learner + 1}\")\n",
    "        return chosen_learner\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.datasets\n",
    "import sklearn.linear_model\n",
    "import sklearn.tree\n",
    "import openml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "openmlid =11\n",
    "X, y = sklearn.datasets.fetch_openml(data_id=openmlid, return_X_y=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 1. 1. 1.]\n",
      " [1. 1. 1. 2.]\n",
      " [1. 1. 1. 3.]\n",
      " ...\n",
      " [5. 5. 5. 3.]\n",
      " [5. 5. 5. 4.]\n",
      " [5. 5. 5. 5.]] ['B' 'R' 'R' 'R' 'R' 'R' 'R' 'R' 'R' 'R' 'R' 'R' 'R' 'R' 'R' 'R' 'R' 'R'\n",
      " 'R' 'R' 'R' 'R' 'R' 'R' 'R' 'L' 'B' 'R' 'R' 'R' 'B' 'R' 'R' 'R' 'R' 'R'\n",
      " 'R' 'R' 'R' 'R' 'R' 'R' 'R' 'R' 'R' 'R' 'R' 'R' 'R' 'R' 'L' 'L' 'B' 'R'\n",
      " 'R' 'L' 'R' 'R' 'R' 'R' 'B' 'R' 'R' 'R' 'R' 'R' 'R' 'R' 'R' 'R' 'R' 'R'\n",
      " 'R' 'R' 'R' 'L' 'L' 'L' 'B' 'R' 'L' 'B' 'R' 'R' 'R' 'L' 'R' 'R' 'R' 'R'\n",
      " 'B' 'R' 'R' 'R' 'R' 'R' 'R' 'R' 'R' 'R' 'L' 'L' 'L' 'L' 'B' 'L' 'L' 'R'\n",
      " 'R' 'R' 'L' 'R' 'R' 'R' 'R' 'L' 'R' 'R' 'R' 'R' 'B' 'R' 'R' 'R' 'R' 'L'\n",
      " 'B' 'R' 'R' 'R' 'B' 'R' 'R' 'R' 'R' 'R' 'R' 'R' 'R' 'R' 'R' 'R' 'R' 'R'\n",
      " 'R' 'R' 'R' 'R' 'R' 'R' 'L' 'L' 'L' 'B' 'R' 'L' 'B' 'R' 'R' 'R' 'L' 'R'\n",
      " 'R' 'R' 'R' 'B' 'R' 'R' 'R' 'R' 'R' 'R' 'R' 'R' 'R' 'L' 'L' 'L' 'L' 'L'\n",
      " 'L' 'L' 'B' 'R' 'R' 'L' 'B' 'R' 'R' 'R' 'L' 'R' 'R' 'R' 'R' 'L' 'R' 'R'\n",
      " 'R' 'R' 'L' 'L' 'L' 'L' 'L' 'L' 'L' 'L' 'B' 'R' 'L' 'L' 'R' 'R' 'R' 'L'\n",
      " 'B' 'R' 'R' 'R' 'L' 'R' 'R' 'R' 'R' 'L' 'L' 'L' 'L' 'L' 'L' 'L' 'L' 'L'\n",
      " 'B' 'L' 'L' 'L' 'R' 'R' 'L' 'L' 'R' 'R' 'R' 'L' 'B' 'R' 'R' 'R' 'L' 'L'\n",
      " 'B' 'R' 'R' 'L' 'R' 'R' 'R' 'R' 'B' 'R' 'R' 'R' 'R' 'R' 'R' 'R' 'R' 'R'\n",
      " 'R' 'R' 'R' 'R' 'R' 'L' 'L' 'L' 'L' 'L' 'L' 'L' 'B' 'R' 'R' 'L' 'B' 'R'\n",
      " 'R' 'R' 'L' 'R' 'R' 'R' 'R' 'L' 'R' 'R' 'R' 'R' 'L' 'L' 'L' 'L' 'L' 'L'\n",
      " 'L' 'L' 'L' 'R' 'L' 'L' 'B' 'R' 'R' 'L' 'L' 'R' 'R' 'R' 'L' 'R' 'R' 'R'\n",
      " 'R' 'L' 'L' 'L' 'L' 'L' 'L' 'L' 'L' 'L' 'L' 'L' 'L' 'L' 'B' 'R' 'L' 'L'\n",
      " 'B' 'R' 'R' 'L' 'L' 'R' 'R' 'R' 'L' 'L' 'L' 'L' 'L' 'L' 'L' 'L' 'L' 'L'\n",
      " 'L' 'L' 'L' 'L' 'B' 'L' 'L' 'L' 'R' 'R' 'L' 'L' 'B' 'R' 'R' 'L' 'L' 'L'\n",
      " 'B' 'R' 'L' 'B' 'R' 'R' 'R' 'L' 'R' 'R' 'R' 'R' 'B' 'R' 'R' 'R' 'R' 'R'\n",
      " 'R' 'R' 'R' 'R' 'L' 'L' 'L' 'L' 'L' 'L' 'L' 'L' 'B' 'R' 'L' 'L' 'R' 'R'\n",
      " 'R' 'L' 'B' 'R' 'R' 'R' 'L' 'R' 'R' 'R' 'R' 'L' 'L' 'L' 'L' 'L' 'L' 'L'\n",
      " 'L' 'L' 'L' 'L' 'L' 'L' 'B' 'R' 'L' 'L' 'B' 'R' 'R' 'L' 'L' 'R' 'R' 'R'\n",
      " 'L' 'L' 'L' 'L' 'L' 'L' 'L' 'L' 'L' 'L' 'L' 'L' 'L' 'L' 'L' 'L' 'L' 'L'\n",
      " 'B' 'R' 'L' 'L' 'L' 'R' 'R' 'L' 'L' 'L' 'L' 'L' 'L' 'L' 'L' 'L' 'L' 'L'\n",
      " 'L' 'L' 'L' 'L' 'L' 'L' 'L' 'L' 'B' 'L' 'L' 'L' 'B' 'R' 'L' 'L' 'L' 'L'\n",
      " 'B' 'L' 'L' 'R' 'R' 'R' 'L' 'R' 'R' 'R' 'R' 'L' 'R' 'R' 'R' 'R' 'B' 'R'\n",
      " 'R' 'R' 'R' 'L' 'L' 'L' 'L' 'L' 'L' 'L' 'L' 'L' 'B' 'L' 'L' 'L' 'R' 'R'\n",
      " 'L' 'L' 'R' 'R' 'R' 'L' 'B' 'R' 'R' 'R' 'L' 'L' 'L' 'L' 'L' 'L' 'L' 'L'\n",
      " 'L' 'L' 'L' 'L' 'L' 'L' 'B' 'L' 'L' 'L' 'R' 'R' 'L' 'L' 'B' 'R' 'R' 'L'\n",
      " 'L' 'L' 'L' 'L' 'L' 'L' 'L' 'L' 'L' 'L' 'L' 'L' 'L' 'L' 'L' 'L' 'L' 'L'\n",
      " 'B' 'L' 'L' 'L' 'B' 'R' 'L' 'L' 'L' 'L' 'L' 'L' 'L' 'L' 'L' 'L' 'L' 'L'\n",
      " 'L' 'L' 'L' 'L' 'L' 'L' 'L' 'L' 'L' 'L' 'L' 'L' 'B']\n"
     ]
    }
   ],
   "source": [
    "print(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read in data frame. Size is 625 x 5.\n",
      "Data is of shape (625, 4).\n",
      "[[1. 1. 1. 1.]\n",
      " [1. 1. 1. 2.]\n",
      " [1. 1. 1. 3.]\n",
      " ...\n",
      " [5. 5. 5. 3.]\n",
      " [5. 5. 5. 4.]\n",
      " [5. 5. 5. 5.]] [0 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1 0 2 2 2 0 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 1 1 0 2 2 1 2 2 2 2 0 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 1 1 1 0 2 1 0 2 2 2 1 2 2 2 2 0 2 2 2 2 2 2 2 2 2 1 1 1 1 0 1 1 2 2 2 1\n",
      " 2 2 2 2 1 2 2 2 2 0 2 2 2 2 1 0 2 2 2 0 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 1 1 1 0 2 1 0 2 2 2 1 2 2 2 2 0 2 2 2 2 2 2 2 2 2 1 1 1 1 1 1 1 0 2 2\n",
      " 1 0 2 2 2 1 2 2 2 2 1 2 2 2 2 1 1 1 1 1 1 1 1 0 2 1 1 2 2 2 1 0 2 2 2 1 2\n",
      " 2 2 2 1 1 1 1 1 1 1 1 1 0 1 1 1 2 2 1 1 2 2 2 1 0 2 2 2 1 1 0 2 2 1 2 2 2\n",
      " 2 0 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1 1 1 1 1 1 1 0 2 2 1 0 2 2 2 1 2 2 2 2 1\n",
      " 2 2 2 2 1 1 1 1 1 1 1 1 1 2 1 1 0 2 2 1 1 2 2 2 1 2 2 2 2 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 0 2 1 1 0 2 2 1 1 2 2 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 2 2\n",
      " 1 1 0 2 2 1 1 1 0 2 1 0 2 2 2 1 2 2 2 2 0 2 2 2 2 2 2 2 2 2 1 1 1 1 1 1 1\n",
      " 1 0 2 1 1 2 2 2 1 0 2 2 2 1 2 2 2 2 1 1 1 1 1 1 1 1 1 1 1 1 1 0 2 1 1 0 2\n",
      " 2 1 1 2 2 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 2 1 1 1 2 2 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 0 2 1 1 1 1 0 1 1 2 2 2 1 2 2 2 2 1 2 2\n",
      " 2 2 0 2 2 2 2 1 1 1 1 1 1 1 1 1 0 1 1 1 2 2 1 1 2 2 2 1 0 2 2 2 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 0 1 1 1 2 2 1 1 0 2 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 0 1 1 1 0 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-3-81af03631861>:36: FutureWarning: Starting from Version 0.15 `download_data`, `download_qualities`, and `download_features_meta_data` will all be ``False`` instead of ``True`` by default to enable lazy loading. To disable this message until version 0.15 explicitly set `download_data`, `download_qualities`, and `download_features_meta_data` to a bool while calling `get_dataset`.\n",
      "  ds = openml.datasets.get_dataset(openmlid)\n"
     ]
    }
   ],
   "source": [
    "X,y=get_dataset(11)\n",
    "print(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "selector=VerticalEvaluator(X,y,[11],\"cv\",train_size=0.8,timeout_per_evaluation=5,epsilon = 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-3-81af03631861>:106: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
      "  h1_before, h2_before = hash(X_train.tostring()), hash(X_test.tostring())\n",
      "<ipython-input-3-81af03631861>:117: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
      "  h1_after, h2_after = hash(X_train.tostring()), hash(X_test.tostring())\n",
      "<ipython-input-3-81af03631861>:106: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
      "  h1_before, h2_before = hash(X_train.tostring()), hash(X_test.tostring())\n",
      "<ipython-input-3-81af03631861>:117: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
      "  h1_after, h2_after = hash(X_train.tostring()), hash(X_test.tostring())\n",
      "<ipython-input-3-81af03631861>:106: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
      "  h1_before, h2_before = hash(X_train.tostring()), hash(X_test.tostring())\n",
      "<ipython-input-3-81af03631861>:117: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
      "  h1_after, h2_after = hash(X_train.tostring()), hash(X_test.tostring())\n",
      "<ipython-input-3-81af03631861>:106: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
      "  h1_before, h2_before = hash(X_train.tostring()), hash(X_test.tostring())\n",
      "<ipython-input-3-81af03631861>:117: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
      "  h1_after, h2_after = hash(X_train.tostring()), hash(X_test.tostring())\n",
      "<ipython-input-3-81af03631861>:106: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
      "  h1_before, h2_before = hash(X_train.tostring()), hash(X_test.tostring())\n",
      "<ipython-input-3-81af03631861>:117: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
      "  h1_after, h2_after = hash(X_train.tostring()), hash(X_test.tostring())\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.09280000000000002"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "pipe = Pipeline([('scaler', StandardScaler()), ('svc', SVC())])\n",
    "selector.cv(pipe,0,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nan"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selector.lccv80(pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-3-81af03631861>:106: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
      "  h1_before, h2_before = hash(X_train.tostring()), hash(X_test.tostring())\n",
      "<ipython-input-3-81af03631861>:117: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
      "  h1_after, h2_after = hash(X_train.tostring()), hash(X_test.tostring())\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'VerticalEvaluator' object has no attribute 'best_observations'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-68-1378beded15d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mselector\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwilcoxon\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpipe\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-3-81af03631861>\u001b[0m in \u001b[0;36mwilcoxon\u001b[1;34m(self, pl, seed, folds)\u001b[0m\n\u001b[0;32m    362\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    363\u001b[0m             \u001b[1;31m# now conduct a wilcoxon signed rank test to determine whether significance has been reached\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 364\u001b[1;33m             \u001b[0mscores_currently_best\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_observations\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_observations\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mones\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    365\u001b[0m             \u001b[0meval_logger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"Currently best observations after {inner_run + 1} evaluations: {np.round(scores_currently_best, 2)}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    366\u001b[0m             \u001b[0meval_logger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"Current cand.  observations after {inner_run + 1} evaluations: {np.round(scores, 2)}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'VerticalEvaluator' object has no attribute 'best_observations'"
     ]
    }
   ],
   "source": [
    "selector.wilcoxon(pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class VerticalEvaluator in module __main__:\n",
      "\n",
      "class VerticalEvaluator(Evaluator)\n",
      " |  VerticalEvaluator(X, y, binarize_sparse, validation, train_size, timeout_per_evaluation, epsilon, seed=0, exception_on_failure=False, other_args={})\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      VerticalEvaluator\n",
      " |      Evaluator\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, X, y, binarize_sparse, validation, train_size, timeout_per_evaluation, epsilon, seed=0, exception_on_failure=False, other_args={})\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  cv(self, pl, seed, folds)\n",
      " |  \n",
      " |  lccv80(self, pl, seed=None)\n",
      " |  \n",
      " |  lccv80flex(self, pl, seed=None)\n",
      " |  \n",
      " |  lccv90(self, pl, seed)\n",
      " |  \n",
      " |  lccv90flex(self, pl, seed=None)\n",
      " |  \n",
      " |  select_model(self, learners, errors='ignore')\n",
      " |  \n",
      " |  wilcoxon(self, pl, seed=0, folds=10)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from Evaluator:\n",
      " |  \n",
      " |  eval_pipeline_on_fold(self, pl, X_train, X_test, y_train, y_test, timeout=None)\n",
      " |  \n",
      " |  get_pipeline_from_descriptor(self, learner)\n",
      " |  \n",
      " |  get_result_of_cv(self, folds, seed=None, timeout=None)\n",
      " |  \n",
      " |  mccv(self, learner, target_size=0.9, timeout=None, seed=0, repeats=10)\n",
      " |      Conducts a 90/10 MCCV (imitating a bit a 10-fold cross validation)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from Evaluator:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(VerticalEvaluator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "eval_pipeline_on_fold() missing 4 required positional arguments: 'X_train', 'X_test', 'y_train', and 'y_test'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-36-cc872364c8c0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mselector\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meval_pipeline_on_fold\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpipe\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: eval_pipeline_on_fold() missing 4 required positional arguments: 'X_train', 'X_test', 'y_train', and 'y_test'"
     ]
    }
   ],
   "source": [
    "selector.eval_pipeline_on_fold(pipe)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import openml\n",
    "import lccv\n",
    "import os, psutil\n",
    "import gc\n",
    "import logging\n",
    "import traceback\n",
    "\n",
    "from func_timeout import func_timeout, FunctionTimedOut\n",
    "import time\n",
    "import random\n",
    "import itertools as it\n",
    "import scipy.stats\n",
    "from scipy.sparse import lil_matrix\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn import metrics\n",
    "from sklearn import *\n",
    "from tqdm import tqdm\n",
    "\n",
    "import import_ipynb\n",
    "import Commons\n",
    "\n",
    "eval_logger = logging.getLogger(\"evalutils\")\n",
    "\n",
    "\n",
    "def get_dataset(openmlid):\n",
    "    ds = openml.datasets.get_dataset(openmlid)\n",
    "    df = ds.get_data()[0]\n",
    "    num_rows = len(df)\n",
    "        \n",
    "    # prepare label column as numpy array\n",
    "    print(f\"Read in data frame. Size is {len(df)} x {len(df.columns)}.\")\n",
    "    X = np.array(df.drop(columns=[ds.default_target_attribute]).values)\n",
    "    y = np.array(df[ds.default_target_attribute].values)\n",
    "    if y.dtype != int:\n",
    "        y_int = np.zeros(len(y)).astype(int)\n",
    "        vals = np.unique(y)\n",
    "        for i, val in enumerate(vals):\n",
    "            mask = y == val\n",
    "            y_int[mask] = i\n",
    "        y = y_int\n",
    "        \n",
    "    print(f\"Data is of shape {X.shape}.\")\n",
    "    return X, y\n",
    "\n",
    "\n",
    "def format_learner(learner):\n",
    "    learner_name = str(learner).replace(\"\\n\", \" \").replace(\"\\t\", \" \")\n",
    "    for k in  range(20):\n",
    "        learner_name = learner_name.replace(\"  \", \" \")\n",
    "    return learner_name\n",
    "\n",
    "\n",
    "def decide_block_train(pl, anchor):\n",
    "    steps = pl.steps\n",
    "    predictor = steps[-1][1]\n",
    "    if type(predictor) == sklearn.ensemble.HistGradientBoostingClassifier:\n",
    "        min_samples_leaf = predictor.min_samples_leaf\n",
    "        return anchor >= 2 * min_samples_leaf\n",
    "    \n",
    "    return True\n",
    "\n",
    "\n",
    "class Evaluator:\n",
    "    \n",
    "    def __init__(self, X, y, binarize_sparse=False):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.best_observations = None\n",
    "        # determine fixed pre-processing steps for imputation and binarization\n",
    "        types = [set([type(v) for v in r]) for r in X.T]\n",
    "        numeric_features = [c for c, t in enumerate(types) if len(t) == 1 and list(t)[0] != str]\n",
    "        numeric_transformer = Pipeline([(\"imputer\", sklearn.impute.SimpleImputer(strategy=\"median\"))])\n",
    "        categorical_features = [i for i in range(X.shape[1]) if i not in numeric_features]\n",
    "        missing_values_per_feature = np.sum(pd.isnull(X), axis=0)\n",
    "        eval_logger.info(f\"There are {len(categorical_features)} categorical features, which will be binarized.\")\n",
    "        eval_logger.info(f\"Missing values for the different attributes are {missing_values_per_feature}.\")\n",
    "        if len(categorical_features) > 0 or sum(missing_values_per_feature) > 0:\n",
    "            categorical_transformer = Pipeline([\n",
    "                (\"imputer\", sklearn.impute.SimpleImputer(strategy=\"most_frequent\")),\n",
    "                (\"binarizer\", sklearn.preprocessing.OneHotEncoder(handle_unknown='ignore', sparse=binarize_sparse)),\n",
    "            ])\n",
    "            self.mandatory_pre_processing = [(\"impute_and_binarize\", ColumnTransformer(\n",
    "                transformers=[\n",
    "                    (\"num\", numeric_transformer, numeric_features),\n",
    "                    (\"cat\", categorical_transformer, categorical_features),\n",
    "                ]\n",
    "            ))]\n",
    "        else:\n",
    "            self.mandatory_pre_processing = []\n",
    "    \n",
    "    def eval_pipeline_on_fold(self, pl, X_train, X_test, y_train, y_test, timeout=None):\n",
    "        try:\n",
    "            pl = Pipeline(self.mandatory_pre_processing + sklearn.base.clone(pl).steps)\n",
    "            \n",
    "            h1_before, h2_before = hash(X_train.tostring()), hash(X_test.tostring())\n",
    "            if timeout is None:\n",
    "                eval_logger.info(f\"Fitting model with {X_train.shape[0]} instances and without timeout.\")\n",
    "                pl.fit(X_train, y_train)\n",
    "            else:\n",
    "                eval_logger.info(f\"Fitting model with {X_train.shape[0]} instances and timeout {timeout}.\")\n",
    "                func_timeout(timeout, pl.fit, (X_train, y_train))\n",
    "                \n",
    "            y_hat = pl.predict(X_test)\n",
    "            error_rate = 1 - sklearn.metrics.accuracy_score(y_test, y_hat)\n",
    "            eval_logger.info(f\"Observed an error rate of {error_rate}\")\n",
    "            h1_after, h2_after = hash(X_train.tostring()), hash(X_test.tostring())\n",
    "            if h1_before != h1_after or h2_before != h2_after:\n",
    "                raise Exception(\"Pipeline has modified the original data, which is forbidden!\")\n",
    "            return error_rate\n",
    "        \n",
    "        except FunctionTimedOut:\n",
    "            eval_logger.info(f\"Timeout observed for evaluation, stopping and returning nan.\")\n",
    "        except KeyboardInterrupt:\n",
    "            raise\n",
    "        except Exception as e:\n",
    "            eval_logger.info(f\"Observed some exception. Stopping. Exception: {e}\")\n",
    "        \n",
    "        return np.nan\n",
    "    \n",
    "    def mccv(self, learner, target_size=.9, timeout=None, seed=0, repeats=10):\n",
    "\n",
    "        \"\"\"\n",
    "        Conducts a 90/10 MCCV (imitating a bit a 10-fold cross validation)\n",
    "        \"\"\"\n",
    "        eval_logger.info(f\"Running mccv with seed  {seed}\")\n",
    "        if not timeout is None:\n",
    "            deadline = time.time() + timeout\n",
    "\n",
    "        scores = []\n",
    "        n = self.X.shape[0]\n",
    "        num_examples = int(target_size * n)\n",
    "        deadline = None if timeout is None else time.time() + timeout\n",
    "\n",
    "        seed *= 13\n",
    "        for r in range(repeats):\n",
    "            eval_logger.info(f\"Seed in MCCV: {seed}. Training on {num_examples} examples. That is {np.round(100 * num_examples / n, 2)} percent of the data.\")\n",
    "            random.seed(seed)\n",
    "            idx = list(range(n))\n",
    "            random.shuffle(idx)\n",
    "            idx_train, idx_test = idx[:num_examples], idx[num_examples:]\n",
    "            X_train, X_test = self.X[idx_train], self.X[idx_test]\n",
    "            y_train, y_test = self.y[idx_train], self.y[idx_test]\n",
    "            scores.append(self.eval_pipeline_on_fold(learner, X_train, X_test, y_train, y_test, timeout))\n",
    "            if not deadline is None and time.time() >= deadline:\n",
    "                eval_logger.info(f\"Deadline reached in MCCV. Stopping.\")\n",
    "                break\n",
    "            \n",
    "            seed += 1\n",
    "        \n",
    "        return np.mean(scores)\n",
    "    \n",
    "\n",
    "class VerticalEvaluator(Evaluator):\n",
    "    \n",
    "    def __init__(self, X, y, binarize_sparse, validation, train_size, timeout_per_evaluation, epsilon, seed=0, exception_on_failure=False, other_args={}, best_observations=None):\n",
    "        super().__init__(X, y, binarize_sparse)\n",
    "        self.r = 0\n",
    "        self.best_observations = best_observations\n",
    "        self.other_args = other_args\n",
    "        if validation == \"cv\":\n",
    "            if train_size == 0.8:\n",
    "                num_folds = 5\n",
    "            elif train_size == 0.9:\n",
    "                num_folds = 10\n",
    "            else:\n",
    "                raise ValueError(f\"Cannot run cross-validation for train_size {train_size}. Must be 0.8 or 0.9.\")\n",
    "            self.validation_func = lambda pl, seed: self.cv(pl, seed, num_folds, *self.other_args)\n",
    "        elif \"lccv\" in validation:\n",
    "            is_flex = \"flex\" in validation\n",
    "            self.r = 1.0\n",
    "            if train_size == 0.8:\n",
    "                self.validation_func = self.lccv80flex if is_flex else self.lccv80\n",
    "            elif train_size == 0.9:\n",
    "                self.validation_func = self.lccv90flex if is_flex else self.lccv90\n",
    "            else:\n",
    "                raise ValueError(f\"Cannot run LCCV for train_size {train_size}. Must be 0.8 or 0.9.\")\n",
    "        elif validation == \"wilcoxon\":\n",
    "            self.r = 1.0\n",
    "            if train_size == 0.8:\n",
    "                self.validation_func = lambda pl, seed: self.wilcoxon(pl, seed=seed, folds=5)\n",
    "            elif train_size == 0.9:\n",
    "                self.validation_func = lambda pl, seed: self.wilcoxon(pl, seed=seed, folds=10)\n",
    "            else:\n",
    "                raise ValueError(f\"Cannot run Wilcoxon for train_size {train_size}. Must be 0.8 or 0.9.\")\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported validation function {validation}.\")\n",
    "        self.timeout_per_evaluation = timeout_per_evaluation\n",
    "        self.epsilon = epsilon\n",
    "        self.seed = seed\n",
    "        self.exception_on_failure = exception_on_failure\n",
    "    \n",
    "    def evaluate(self, learners, timeout=None):\n",
    "        observations = []\n",
    "        for learner in learners:\n",
    "            if not self.exception_on_failure:\n",
    "                try:\n",
    "                    observation = self.evaluate_learner(learner, timeout)\n",
    "                    if not observation is None:\n",
    "                        observations.append(observation)\n",
    "                except KeyboardInterrupt:\n",
    "                    raise\n",
    "                except Exception as e:\n",
    "                    traceback.print_exc()\n",
    "                    eval_logger.info(f\"Encountered an exception: {e}\")\n",
    "            else:\n",
    "                observation = self.evaluate_learner(learner, timeout)\n",
    "                if not observation is None:\n",
    "                    observations.append(observation)\n",
    "        return observations\n",
    "    \n",
    "    def evaluate_learner(self, learner, timeout=None):\n",
    "        seed = self.seed\n",
    "        observations = []\n",
    "        anchor = 1.0\n",
    "        \n",
    "        while True:\n",
    "            obs = self.validation_func(learner, seed)\n",
    "            if obs is None:\n",
    "                break\n",
    "            if obs[\"error_rate\"] <= anchor - self.epsilon:\n",
    "                anchor = obs[\"error_rate\"]\n",
    "                eval_logger.info(f\"Anchor is now {anchor}. Observation: {obs}\")\n",
    "                self.best_observations = [obs]\n",
    "            elif obs[\"error_rate\"] <= anchor + self.epsilon:\n",
    "                eval_logger.info(f\"Within epsilon. Observation: {obs}\")\n",
    "                self.best_observations.append(obs)\n",
    "            else:\n",
    "                eval_logger.info(f\"Stopping search as the error rate is too high {obs['error_rate']} and above anchor {anchor}.\")\n",
    "                break\n",
    "\n",
    "            seed += 1\n",
    "        \n",
    "        return observations\n",
    "    \n",
    "    def cv(self, pl, seed, num_folds):\n",
    "        return {\"error_rate\": 1 - np.mean(sklearn.model_selection.cross_val_score(pl, self.X, self.y, cv=num_folds, n_jobs=-1))}\n",
    "\n",
    "    def lccv80(self, pl, seed):\n",
    "        eval_logger.info(f\"Running LCCV80 with seed {seed}\")\n",
    "        return {\"error_rate\": 1 - mccv(pl, .8, 300, seed)}\n",
    "\n",
    "    def lccv90(self, pl, seed):\n",
    "        eval_logger.info(f\"Running LCCV90 with seed {seed}\")\n",
    "        return {\"error_rate\": 1 - mccv(pl, .9, 300, seed)}\n",
    "\n",
    "    def lccv80flex(self, pl, seed):\n",
    "        eval_logger.info(f\"Running LCCV80flex with seed {seed}\")\n",
    "        return {\"error_rate\": 1 - mccv(pl, .8, 300, seed)}\n",
    "\n",
    "    def lccv90flex(self, pl, seed):\n",
    "        eval_logger.info(f\"Running LCCV90flex with seed {seed}\")\n",
    "        return {\"error_rate\": 1 - mccv(pl, .9, 300, seed)}\n",
    "\n",
    "    def wilcoxon(self, pl, seed, folds):\n",
    "        eval_logger.info(f\"Running Wilcoxon with seed {seed}\")\n",
    "        eval_logger.info(f\"Number of folds in Wilcoxon is {folds}.\")\n",
    "        results = [mccv(pl, .8, 300, seed + i, repeats=folds) for i in range(10)]\n",
    "        return {\"error_rate\": np.mean(results)}\n",
    "\n",
    "def mccv(learner, target_size=.9, timeout=None, seed=0, repeats=10):\n",
    "\n",
    "    \"\"\"\n",
    "    Conducts a 90/10 MCCV (imitating a bit a 10-fold cross validation)\n",
    "    \"\"\"\n",
    "    eval_logger.info(f\"Running mccv with seed  {seed}\")\n",
    "    if not timeout is None:\n",
    "        deadline = time.time() + timeout\n",
    "\n",
    "    scores = []\n",
    "    n = self.X.shape[0]\n",
    "    num_examples = int(target_size * n)\n",
    "    deadline = None if timeout is None else time.time() + timeout\n",
    "\n",
    "    seed *= 13\n",
    "    for r in range(repeats):\n",
    "        eval_logger.info(f\"Seed in MCCV: {seed}. Training on {num_examples} examples. That is {np.round(100 * num_examples / n, 2)} percent of the data.\")\n",
    "        random.seed(seed)\n",
    "        idx = list(range(n))\n",
    "        random.shuffle(idx)\n",
    "        idx_train, idx_test = idx[:num_examples], idx[num_examples:]\n",
    "        X_train, X_test = self.X[idx_train], self.X[idx_test]\n",
    "        y_train, y_test = self.y[idx_train], self.y[idx_test]\n",
    "        scores.append(self.eval_pipeline_on_fold(learner, X_train, X_test, y_train, y_test, timeout))\n",
    "        if not deadline is None and time.time() >= deadline:\n",
    "            eval_logger.info(f\"Deadline reached in MCCV. Stopping.\")\n",
    "            break\n",
    "        \n",
    "        seed += 1\n",
    "    \n",
    "    return np.mean(scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
