{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openml\n",
    "import pandas as pd\n",
    "\n",
    "import sklearn as sk\n",
    "import sklearn.ensemble\n",
    "import sklearn.decomposition\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.cluster import SpectralClustering\n",
    "from sklearn import metrics\n",
    "from sklearn import *\n",
    "from scipy.sparse import lil_matrix\n",
    "\n",
    "import pebble\n",
    "from func_timeout import func_timeout, FunctionTimedOut\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "# import ConfigSpace\n",
    "# from ConfigSpace.util import *\n",
    "# from ConfigSpace.read_and_write import json as config_json\n",
    "import json\n",
    "\n",
    "import itertools as it\n",
    "\n",
    "import os, psutil\n",
    "import multiprocessing\n",
    "from concurrent.futures import TimeoutError\n",
    "\n",
    "import scipy.sparse\n",
    "\n",
    "# We sub-class multiprocessing.pool.Pool instead of multiprocessing.Pool\n",
    "# because the latter is only a wrapper function, not a proper class.\n",
    "class NestablePool(multiprocessing.pool.Pool):\n",
    "\tdef __init__(self, *args, **kwargs):\n",
    "\t    kwargs['context'] = NoDaemonContext()\n",
    "\t    super(NestablePool, self).__init__(*args, **kwargs)\n",
    "\t    print(\"Established a nestable pool with params: \" + str(*args))\n",
    "\n",
    "class NoDaemonProcess(multiprocessing.Process):\n",
    "    @property\n",
    "    def daemon(self):\n",
    "        return False\n",
    "\n",
    "    @daemon.setter\n",
    "    def daemon(self, value):\n",
    "        pass\n",
    "        \n",
    "class NoDaemonContext(type(multiprocessing.get_context(\"spawn\"))):\n",
    "\tProcess = NoDaemonProcess\n",
    "        \n",
    "def get_class( kls ):\n",
    "    parts = kls.split('.')\n",
    "    module = \".\".join(parts[:-1])\n",
    "    m = __import__( module )\n",
    "    for comp in parts[1:]:\n",
    "        m = getattr(m, comp)            \n",
    "    return m\n",
    "\n",
    "def is_component_defined_in_steps(steps, name):\n",
    "    candidates = [s[1] for s in steps if s[0] == name]\n",
    "    return len(candidates) > 0\n",
    "\n",
    "def get_step_with_name(steps, name):\n",
    "    candidates = [s for s in steps if s[0] == name]\n",
    "    return candidates[0]\n",
    "\n",
    "class EvaluationPool:\n",
    "\n",
    "    def __init__(self, X, y, scoring, tolerance_tuning = 0.05, tolerance_estimation_error = 0.01):\n",
    "        if X is None:\n",
    "            raise Exception(\"Parameter X must not be None\")\n",
    "        if y is None:\n",
    "            raise Exception(\"Parameter y must not be None\")\n",
    "        if type(X) != np.ndarray and type(X) != scipy.sparse.csr.csr_matrix and type(X) != scipy.sparse.lil.lil_matrix:\n",
    "            raise Exception(\"X must be a numpy array but is \" + str(type(X)))\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.scoring = scoring\n",
    "        self.bestScore = -np.inf\n",
    "        self.tolerance_tuning = tolerance_tuning\n",
    "        self.tolerance_estimation_error = tolerance_estimation_error\n",
    "        self.cache = {}\n",
    "        \n",
    "    def merge(self, pool):\n",
    "        num_entries_before = len(self.cache)\n",
    "        for spl in pool.cache:\n",
    "            pl, learning_curve, timestamp = pool.cache[spl]\n",
    "            self.tellEvaluation(pl, learning_curve, timestamp)\n",
    "        num_entries_after = len(self.cache)\n",
    "        print(\"Adopted \" + str(num_entries_after - num_entries_before) + \" entries from external pool.\")\n",
    "\n",
    "    def tellEvaluation(self, pl, scores, timestamp):\n",
    "        spl = str(pl)\n",
    "        self.cache[spl] = (pl, scores, timestamp)\n",
    "        score = np.mean(scores)\n",
    "        if score > self.bestScore:\n",
    "            self.bestScore = score        \n",
    "            self.best_spl = spl\n",
    "            \n",
    "    def cross_validate(self, pl, X, y, scoring): # just a wrapper to ease parallelism\n",
    "        try:\n",
    "            scorer = sklearn.metrics.make_scorer(sklearn.metrics.log_loss if scoring == \"neg_log_loss\" else sklearn.metrics.roc_auc_score, greater_is_better = scoring != \"neg_log_loss\", needs_proba=True, labels=list(np.unique(y)))\n",
    "            return sklearn.model_selection.cross_validate(pl, X, y, scoring=scorer, error_score=\"raise\")\n",
    "        except:\n",
    "            raise\n",
    "            #print(\"OBSERVED ERROR, EXECUTION ABORTED!\")\n",
    "            #return None\n",
    "\n",
    "    def evaluate(self, pl, timeout=None, deadline=None, verbose=False):\n",
    "        if is_pipeline_forbidden(pl):\n",
    "            if verbose:\n",
    "                print(\"Preventing evaluation of forbidden pipeline \" + str(pl))\n",
    "            return np.nan\n",
    "        \n",
    "        process = psutil.Process(os.getpid())\n",
    "        if verbose:\n",
    "            print(datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"), \"Initializing evaluation of \" + str(pl) + \" with current memory consumption \" + str(int(process.memory_info().rss / 1024 / 1024))  + \"MB. Now awaiting results.\")\n",
    "        \n",
    "        start_outer = time.time()\n",
    "        spl = str(pl)\n",
    "        if spl in self.cache:\n",
    "            return np.round(np.mean(self.cache[spl][1]), 4)\n",
    "        timestamp = time.time()\n",
    "        if timeout is not None:\n",
    "            result = func_timeout(timeout, self.cross_validate, (pl, self.X, self.y, self.scoring))\n",
    "        else:\n",
    "            result = self.cross_validate(pl, self.X, self.y, self.scoring)\n",
    "        if result is None:\n",
    "            return np.nan\n",
    "        scores = result[\"test_score\"]\n",
    "        runtime = time.time() - start_outer\n",
    "        if verbose:\n",
    "            print(\"Completed evaluation of \" + spl + \" after \" + str(runtime) + \"s. Scores are\", scores)\n",
    "        self.tellEvaluation(pl, scores, timestamp)\n",
    "        return np.round(np.mean(scores), 4)\n",
    "\n",
    "    def getBestCandidate(self):\n",
    "        return self.getBestCandidates(1)[0]\n",
    "        \n",
    "    def getBestCandidates(self, n):\n",
    "        candidates = sorted([key for key in self.cache], key=lambda k: np.mean(self.cache[k][1]), reverse=True)\n",
    "        return [self.cache[c] for c in candidates[:n]]\n",
    "    \n",
    "def fullname(o):\n",
    "    module = o.__module__\n",
    "    if module is None or module == str.__class__.__module__:\n",
    "        return o.__name__  # Avoid reporting __builtin__\n",
    "    else:\n",
    "        return module + '.' + o.__name__\n",
    "\n",
    "def check_true(p: str) -> bool:\n",
    "    if p in (\"True\", \"true\", 1, True):\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def check_false(p: str) -> bool:\n",
    "    if p in (\"False\", \"false\", 0, False):\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def check_none(p: str) -> bool:\n",
    "    if p in (\"None\", \"none\", None):\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def check_for_bool(p: str) -> bool:\n",
    "    if check_false(p):\n",
    "        return False\n",
    "    elif check_true(p):\n",
    "        return True\n",
    "    else:\n",
    "        raise ValueError(\"%s is not a bool\" % str(p))\n",
    "    \n",
    "def build_estimator(comp, params, X, y):\n",
    "    \n",
    "    if params is None:\n",
    "        if get_class(comp[\"class\"]) == sklearn.svm.SVC:\n",
    "            params = {\"kernel\": config_json.read(comp[\"params\"]).get_hyperparameter(\"kernel\").value}\n",
    "            print(\"SVC invoked without params. Setting kernel explicitly to \" + params[\"kernel\"])\n",
    "        else:\n",
    "            return get_class(comp[\"class\"])()\n",
    "    \n",
    "    return compile_pipeline_by_class_and_params(get_class(comp[\"class\"]), params, X, y)\n",
    "\n",
    "\n",
    "def compile_pipeline_by_class_and_params(clazz, params, X, y):\n",
    "    \n",
    "    if clazz == sklearn.cluster.FeatureAgglomeration:\n",
    "        pooling_func_mapping = dict(mean=np.mean, median=np.median, max=np.max)\n",
    "        n_clusters = int(params[\"n_clusters\"])\n",
    "        n_clusters = min(n_clusters, X.shape[1])\n",
    "        pooling_func = pooling_func_mapping[params[\"pooling_func\"]]\n",
    "        affinity = params[\"affinity\"]\n",
    "        linkage = params[\"linkage\"]\n",
    "        return sklearn.cluster.FeatureAgglomeration(n_clusters=n_clusters, affinity=affinity, linkage=linkage, pooling_func=pooling_func)\n",
    "    \n",
    "    \n",
    "    \n",
    "    if clazz == sklearn.feature_selection.SelectPercentile:\n",
    "        percentile = int(float(params[\"percentile\"]))\n",
    "        score_func = params[\"score_func\"]\n",
    "        if score_func == \"chi2\":\n",
    "            score_func = sklearn.feature_selection.chi2\n",
    "        elif score_func == \"f_classif\":\n",
    "            score_func = sklearn.feature_selection.f_classif\n",
    "        elif score_func == \"mutual_info\":\n",
    "            score_func = sklearn.feature_selection.mutual_info_classif\n",
    "        else:\n",
    "            raise ValueError(\"score_func must be in ('chi2, 'f_classif', 'mutual_info'), \"\"but is: %s\" % score_func)\n",
    "        return sklearn.feature_selection.SelectPercentile(score_func=score_func, percentile=percentile)\n",
    "    \n",
    "    if clazz == sklearn.preprocessing.RobustScaler:\n",
    "        return sklearn.preprocessing.RobustScaler(quantile_range=(params[\"q_min\"], params[\"q_max\"]), copy=True,)\n",
    "    \n",
    "    if clazz == sklearn.decomposition.PCA:\n",
    "        n_components = float(params[\"keep_variance\"])\n",
    "        whiten = check_for_bool(params[\"whiten\"])\n",
    "        return sklearn.decomposition.PCA(n_components=n_components, whiten=whiten, copy=True)\n",
    "    \n",
    "    if clazz == sklearn.feature_selection.GenericUnivariateSelect:\n",
    "        alpha = params[\"alpha\"]\n",
    "        mode = params[\"mode\"] if \"mode\" in params else None\n",
    "        score_func = params[\"score_func\"]\n",
    "        if score_func == \"chi2\":\n",
    "            score_func = sklearn.feature_selection.chi2\n",
    "        elif score_func == \"f_classif\":\n",
    "            score_func = sklearn.feature_selection.f_classif\n",
    "        elif score_func == \"mutual_info_classif\":\n",
    "            score_func = sklearn.feature_selection.mutual_info_classif\n",
    "            # mutual info classif constantly crashes without mode percentile\n",
    "            mode = 'percentile'\n",
    "        else:\n",
    "            raise ValueError(\"score_func must be in ('chi2, 'f_classif', 'mutual_info_classif') \"\n",
    "                             \"for classification \"\n",
    "                             \"but is: %s \" % (score_func))\n",
    "        return sklearn.feature_selection.GenericUnivariateSelect(score_func=score_func, param=alpha, mode=mode)\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "    if clazz == sklearn.tree.DecisionTreeClassifier:\n",
    "        criterion = params[\"criterion\"]\n",
    "        max_features = float(params[\"max_features\"])\n",
    "        # Heuristic to set the tree depth\n",
    "        if check_none(params[\"max_depth_factor\"]):\n",
    "            max_depth_factor = None\n",
    "        else:\n",
    "            num_features = X.shape[1]\n",
    "            max_depth_factor = int(params[\"max_depth_factor\"])\n",
    "            max_depth_factor = max(\n",
    "                1,\n",
    "                int(np.round(max_depth_factor * num_features, 0)))\n",
    "        min_samples_split = int(params[\"min_samples_split\"])\n",
    "        min_samples_leaf = int(params[\"min_samples_leaf\"])\n",
    "        if check_none(params[\"max_leaf_nodes\"]):\n",
    "            max_leaf_nodes = None\n",
    "        else:\n",
    "            max_leaf_nodes = int(params[\"max_leaf_nodes\"])\n",
    "        min_weight_fraction_leaf = float(params[\"min_weight_fraction_leaf\"])\n",
    "        min_impurity_decrease = float(params[\"min_impurity_decrease\"])\n",
    "\n",
    "        return sklearn.tree.DecisionTreeClassifier(\n",
    "            criterion=criterion,\n",
    "            max_depth=max_depth_factor,\n",
    "            min_samples_split=min_samples_split,\n",
    "            min_samples_leaf=min_samples_leaf,\n",
    "            max_leaf_nodes=max_leaf_nodes,\n",
    "            min_weight_fraction_leaf=min_weight_fraction_leaf,\n",
    "            min_impurity_decrease=min_impurity_decrease,\n",
    "            class_weight=None)\n",
    "    \n",
    "    if clazz == sklearn.svm.LinearSVC:\n",
    "        penalty = params[\"penalty\"]\n",
    "        loss = params[\"loss\"]\n",
    "        multi_class = params[\"multi_class\"]\n",
    "        C = float(params[\"C\"])\n",
    "        tol = float(params[\"tol\"])\n",
    "        dual = check_for_bool(params[\"dual\"])\n",
    "        fit_intercept = check_for_bool(params[\"fit_intercept\"])\n",
    "        intercept_scaling = float(params[\"intercept_scaling\"])\n",
    "            \n",
    "        return sklearn.svm.LinearSVC(penalty=penalty, loss=loss, dual=dual, tol=tol, C=C, fit_intercept=fit_intercept, intercept_scaling=intercept_scaling, multi_class=multi_class)\n",
    "    \n",
    "    if clazz == sklearn.svm.SVC:\n",
    "        kernel = params[\"kernel\"]\n",
    "        if len(params) == 1:\n",
    "            return sklearn.svm.SVC(kernel=kernel, probability=True)\n",
    "        \n",
    "        C = float(params[\"C\"])\n",
    "        if \"degree\" not in params or params[\"degree\"] is None:\n",
    "            degree = 3\n",
    "        else:\n",
    "            degree = int(params[\"degree\"])\n",
    "        if params[\"gamma\"] is None:\n",
    "            gamma = 0.0\n",
    "        else:\n",
    "            gamma = float(params[\"gamma\"])\n",
    "        if \"coef0\" not in params or params[\"coef0\"] is None:\n",
    "            coef0 = 0.0\n",
    "        else:\n",
    "            coef0 = float(params[\"coef0\"])\n",
    "        tol = float(params[\"tol\"])\n",
    "        max_iter = float(params[\"max_iter\"])\n",
    "        shrinking = check_for_bool(params[\"shrinking\"])\n",
    "\n",
    "        return sklearn.svm.SVC(C=C, kernel=kernel,degree=degree,gamma=gamma,coef0=coef0,shrinking=shrinking,tol=tol, max_iter=max_iter, decision_function_shape='ovr', probability=True)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    if clazz == sklearn.discriminant_analysis.LinearDiscriminantAnalysis:\n",
    "        if params[\"shrinkage\"] in (None, \"none\", \"None\"):\n",
    "            shrinkage_ = None\n",
    "            solver = 'svd'\n",
    "        elif params[\"shrinkage\"] == \"auto\":\n",
    "            shrinkage_ = 'auto'\n",
    "            solver = 'lsqr'\n",
    "        elif params[\"shrinkage\"] == \"manual\":\n",
    "            shrinkage_ = float(params[\"shrinkage_factor\"])\n",
    "            solver = 'lsqr'\n",
    "        else:\n",
    "            raise ValueError(self.shrinkage)\n",
    "\n",
    "        tol = float(params[\"tol\"])\n",
    "        return sklearn.discriminant_analysis.LinearDiscriminantAnalysis(shrinkage=shrinkage_, tol=tol, solver=solver)\n",
    "        \n",
    "    if clazz == sklearn.neural_network.MLPClassifier:\n",
    "        _fully_fit = False\n",
    "        max_iter = 512 # hard coded in auto-sklearn\n",
    "        hidden_layer_depth = int(params[\"hidden_layer_depth\"])\n",
    "        num_nodes_per_layer = int(params[\"num_nodes_per_layer\"])\n",
    "        hidden_layer_sizes = tuple(params[\"num_nodes_per_layer\"]\n",
    "                                        for i in range(params[\"hidden_layer_depth\"]))\n",
    "        activation = str(params[\"activation\"])\n",
    "        alpha = float(params[\"alpha\"])\n",
    "        learning_rate_init = float(params[\"learning_rate_init\"])\n",
    "        \n",
    "        early_stopping = \"train\"\n",
    "        \n",
    "        tol = float(params[\"tol\"])\n",
    "        if early_stopping == \"train\":\n",
    "            validation_fraction = 0.0\n",
    "            n_iter_no_change = int(params[\"n_iter_no_change\"])\n",
    "            early_stopping_val = False\n",
    "        elif early_stopping == \"valid\":\n",
    "            validation_fraction = float(params[\"validation_fraction\"])\n",
    "            n_iter_no_change = int(params[\"n_iter_no_change\"])\n",
    "            early_stopping_val = True\n",
    "        else:\n",
    "            raise ValueError(\"Set early stopping to unknown value %s\" % early_stopping)\n",
    "        try:\n",
    "            batch_size = int(params[\"batch_size\"])\n",
    "        except ValueError:\n",
    "            batch_size = str(params[\"batch_size\"])\n",
    "\n",
    "        solver = params[\"solver\"]\n",
    "        shuffle = check_for_bool(params[\"shuffle\"])\n",
    "        beta_1 = float(params[\"beta_1\"])\n",
    "        beta_2 = float(params[\"beta_2\"])\n",
    "        epsilon = float(params[\"epsilon\"])\n",
    "        verbose = False\n",
    "\n",
    "        # initial fit of only increment trees\n",
    "        return sklearn.neural_network.MLPClassifier(\n",
    "            hidden_layer_sizes=hidden_layer_sizes,\n",
    "            activation=activation,\n",
    "            solver=solver,\n",
    "            alpha=alpha,\n",
    "            batch_size=batch_size,\n",
    "            learning_rate_init=learning_rate_init,\n",
    "            max_iter=max_iter,\n",
    "            shuffle=shuffle,\n",
    "            verbose=verbose,\n",
    "            warm_start=False,\n",
    "            early_stopping=early_stopping_val,\n",
    "            validation_fraction=validation_fraction,\n",
    "            n_iter_no_change=n_iter_no_change,\n",
    "            tol=tol,\n",
    "            beta_1=beta_2,\n",
    "            beta_2=beta_1,\n",
    "            epsilon=epsilon\n",
    "        )\n",
    "    \n",
    "    if clazz == sklearn.linear_model.SGDClassifier:\n",
    "        max_iter = 1024\n",
    "        loss = params[\"loss\"]\n",
    "        penalty = params[\"penalty\"]\n",
    "        alpha = float(params[\"alpha\"])\n",
    "        l1_ratio = float(params[\"l1_ratio\"]) if \"l1_ratio\" in params and params[\"l1_ratio\"] is not None else 0.15\n",
    "        epsilon = float(params[\"epsilon\"]) if \"epsilon\" in params and params[\"epsilon\"] is not None else 0.1\n",
    "        eta0 = float(params[\"eta0\"]) if \"eta0\" in params and params[\"eta0\"] is not None else 0.01\n",
    "        power_t = float(params[\"power_t\"]) if \"power_t\" in params and params[\"power_t\"] is not None else 0.5\n",
    "        average = check_for_bool(params[\"average\"])\n",
    "        fit_intercept = check_for_bool(params[\"fit_intercept\"])\n",
    "        tol = float(params[\"tol\"])\n",
    "        learning_rate = params[\"learning_rate\"]\n",
    "\n",
    "        return sklearn.linear_model.SGDClassifier(loss=loss,\n",
    "                                           penalty=penalty,\n",
    "                                           alpha=alpha,\n",
    "                                           fit_intercept=fit_intercept,\n",
    "                                           max_iter=max_iter,\n",
    "                                           tol=tol,\n",
    "                                           learning_rate=learning_rate,\n",
    "                                           l1_ratio=l1_ratio,\n",
    "                                           epsilon=epsilon,\n",
    "                                           eta0=eta0,\n",
    "                                           power_t=power_t,\n",
    "                                           shuffle=True,\n",
    "                                           average=average,\n",
    "                                           warm_start=False)\n",
    "    \n",
    "    \n",
    "    if clazz == sklearn.linear_model.PassiveAggressiveClassifier:\n",
    "        max_iter = 1024 # fixed in auto-sklearn\n",
    "        average = check_for_bool(params[\"average\"])\n",
    "        fit_intercept = check_for_bool(params[\"fit_intercept\"])\n",
    "        tol = float(params[\"tol\"])\n",
    "        C = float(params[\"C\"])\n",
    "        loss = params[\"loss\"]\n",
    "        \n",
    "        return sklearn.linear_model.PassiveAggressiveClassifier(\n",
    "            C=C,\n",
    "            fit_intercept=fit_intercept,\n",
    "            max_iter=max_iter,\n",
    "            tol=tol,\n",
    "            loss=loss,\n",
    "            shuffle=True,\n",
    "            warm_start=False,\n",
    "            average=average,\n",
    "        )\n",
    "        \n",
    "        \n",
    "    if clazz == sklearn.ensemble.RandomForestClassifier:\n",
    "        criterion = params[\"criterion\"]\n",
    "        n_estimators = int(params[\"n_estimators\"]) if \"n_estimators\" in params and params[\"n_estimators\"] is not None else 512\n",
    "        if check_none(params[\"max_depth\"]):\n",
    "            max_depth = None\n",
    "        else:\n",
    "            max_depth = int(params[\"max_depth\"])\n",
    "\n",
    "        min_samples_split = int(params[\"min_samples_split\"])\n",
    "        min_samples_leaf = int(params[\"min_samples_leaf\"])\n",
    "        min_weight_fraction_leaf = float(params[\"min_weight_fraction_leaf\"])\n",
    "\n",
    "        if params[\"max_features\"] not in (\"sqrt\", \"log2\", \"auto\"):\n",
    "            max_features = int(X.shape[1] ** float(params[\"max_features\"]))\n",
    "        else:\n",
    "            max_features = params[\"max_features\"]\n",
    "\n",
    "        bootstrap = check_for_bool(params[\"bootstrap\"])\n",
    "\n",
    "        if check_none(params[\"max_leaf_nodes\"]):\n",
    "            max_leaf_nodes = None\n",
    "        else:\n",
    "            max_leaf_nodes = int(params[\"max_leaf_nodes\"])\n",
    "\n",
    "        min_impurity_decrease = float(params[\"min_impurity_decrease\"])\n",
    "\n",
    "        # initial fit of only increment trees\n",
    "        return sklearn.ensemble.RandomForestClassifier(\n",
    "            n_estimators=n_estimators,\n",
    "            criterion=criterion,\n",
    "            max_features=max_features,\n",
    "            max_depth=max_depth,\n",
    "            min_samples_split=min_samples_split,\n",
    "            min_samples_leaf=min_samples_leaf,\n",
    "            min_weight_fraction_leaf=min_weight_fraction_leaf,\n",
    "            bootstrap=bootstrap,\n",
    "            max_leaf_nodes=max_leaf_nodes,\n",
    "            min_impurity_decrease=min_impurity_decrease,\n",
    "            warm_start=False)\n",
    "    \n",
    "    if clazz == sklearn.ensemble.GradientBoostingClassifier:\n",
    "        from sklearn.experimental import enable_hist_gradient_boosting  # noqa\n",
    "        \n",
    "        params[\"early_stop\"] = \"off\" # always deactivate\n",
    "        \n",
    "        learning_rate = float(params[\"learning_rate\"])\n",
    "        max_iter = int(params[\"max_iter\"]) if \"max_iter\" in params else 512\n",
    "        min_samples_leaf = int(params[\"min_samples_leaf\"])\n",
    "        loss = params[\"loss\"]\n",
    "        scoring = params[\"scoring\"]\n",
    "        if check_none(params[\"max_depth\"]):\n",
    "            max_depth = None\n",
    "        else:\n",
    "            max_depth = int(params[\"max_depth\"])\n",
    "        if check_none(params[\"max_leaf_nodes\"]):\n",
    "            max_leaf_nodes = None\n",
    "        else:\n",
    "            max_leaf_nodes = int(params[\"max_leaf_nodes\"])\n",
    "        max_bins = int(params[\"max_bins\"])\n",
    "        l2_regularization = float(params[\"l2_regularization\"])\n",
    "        tol = float(params[\"tol\"])\n",
    "        if check_none(params[\"scoring\"]):\n",
    "            scoring = None\n",
    "        if params[\"early_stop\"] == \"off\":\n",
    "            n_iter_no_change = 0\n",
    "            validation_fraction_ = None\n",
    "            early_stopping_ = False\n",
    "        elif params[\"early_stop\"] == \"train\":\n",
    "            n_iter_no_change = int(params[\"n_iter_no_change\"])\n",
    "            validation_fraction_ = None\n",
    "            early_stopping_ = True\n",
    "        elif params[\"early_stop\"] == \"valid\":\n",
    "            n_iter_no_change = int(params[\"n_iter_no_change\"])\n",
    "            validation_fraction = float(params[\"validation_fraction\"])\n",
    "            early_stopping_ = True\n",
    "            n_classes = len(np.unique(y))\n",
    "            if validation_fraction * X.shape[0] < n_classes:\n",
    "                validation_fraction_ = n_classes\n",
    "            else:\n",
    "                validation_fraction_ = params[\"validation_fraction\"]\n",
    "        else:\n",
    "            raise ValueError(\"early_stop should be either off, train or valid\")\n",
    "\n",
    "        # initial fit of only increment trees\n",
    "        return sklearn.ensemble.HistGradientBoostingClassifier(\n",
    "            loss=loss,\n",
    "            learning_rate=learning_rate,\n",
    "            max_iter=max_iter,\n",
    "            min_samples_leaf=min_samples_leaf,\n",
    "            max_depth=max_depth,\n",
    "            max_leaf_nodes=max_leaf_nodes,\n",
    "            max_bins=max_bins,\n",
    "            l2_regularization=l2_regularization,\n",
    "            tol=tol,\n",
    "            scoring=scoring,\n",
    "            early_stopping=early_stopping_,\n",
    "            n_iter_no_change=n_iter_no_change,\n",
    "            validation_fraction=validation_fraction_,\n",
    "            warm_start=False\n",
    "        )\n",
    "            \n",
    "            \n",
    "        \n",
    "    if clazz == sklearn.ensemble.ExtraTreesClassifier:\n",
    "        n_estimators = 512\n",
    "        max_features = int(X.shape[1] ** float(params[\"max_features\"]))\n",
    "        if params[\"criterion\"] not in (\"gini\", \"entropy\"):\n",
    "            raise ValueError(\"'criterion' is not in ('gini', 'entropy'): \"\"%s\" % self.criterion)\n",
    "\n",
    "        if check_none(params[\"max_depth\"]):\n",
    "            max_depth = None\n",
    "        else:\n",
    "            max_depth = int(params[\"max_depth\"])\n",
    "        if check_none(params[\"max_leaf_nodes\"]):\n",
    "            max_leaf_nodes = None\n",
    "        else:\n",
    "            max_leaf_nodes = int(params[\"max_leaf_nodes\"])\n",
    "\n",
    "        criterion = params[\"criterion\"]\n",
    "        min_samples_leaf = int(params[\"min_samples_leaf\"])\n",
    "        min_samples_split = int(params[\"min_samples_split\"])\n",
    "        max_features = float(params[\"max_features\"])\n",
    "        min_impurity_decrease = float(params[\"min_impurity_decrease\"])\n",
    "        min_weight_fraction_leaf = float(params[\"min_weight_fraction_leaf\"])\n",
    "        oob_score = check_for_bool(params[\"oob_score\"]) if \"oob_score\" in params else False\n",
    "        bootstrap = check_for_bool(params[\"bootstrap\"])\n",
    "\n",
    "        return sklearn.ensemble.ExtraTreesClassifier(n_estimators=n_estimators,\n",
    "             criterion=criterion,\n",
    "             max_depth=max_depth,\n",
    "             min_samples_split=min_samples_split,\n",
    "             min_samples_leaf=min_samples_leaf,\n",
    "             bootstrap=bootstrap,\n",
    "             max_features=max_features,\n",
    "             max_leaf_nodes=max_leaf_nodes,\n",
    "             min_weight_fraction_leaf=min_weight_fraction_leaf,\n",
    "             min_impurity_decrease=min_impurity_decrease,\n",
    "             oob_score=oob_score,\n",
    "             warm_start=False)\n",
    "        \n",
    "    else:\n",
    "        return clazz(**params)\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "def get_hyperparameter_space_size(config_space):\n",
    "    hps = config_space.get_hyperparameters();\n",
    "    if not hps:\n",
    "        return 0\n",
    "    size = 1\n",
    "    for hp in hps:\n",
    "        #print(hp.name, type(hp))\n",
    "        if type(hp) in [ConfigSpace.hyperparameters.UnParametrizedHyperparameter, ConfigSpace.hyperparameters.Constant]:\n",
    "            continue\n",
    "            \n",
    "        if type(hp) == ConfigSpace.hyperparameters.CategoricalHyperparameter:\n",
    "            size *= len(list(hp.choices))\n",
    "        elif issubclass(hp.__class__, ConfigSpace.hyperparameters.IntegerHyperparameter):\n",
    "            size *= (hp.upper - hp.lower + 1)\n",
    "        else:\n",
    "            return np.inf\n",
    "    return size\n",
    "\n",
    "\n",
    "def get_all_configurations(config_space):\n",
    "    names = []\n",
    "    domains = []\n",
    "    for hp in config_space.get_hyperparameters():\n",
    "        names.append(hp.name)\n",
    "        if type(hp) in [ConfigSpace.hyperparameters.UnParametrizedHyperparameter, ConfigSpace.hyperparameters.Constant]:\n",
    "            domains.append([hp.value])\n",
    "            \n",
    "        if type(hp) == ConfigSpace.hyperparameters.CategoricalHyperparameter:\n",
    "            domains.append(list(hp.choices))\n",
    "        elif issubclass(hp.__class__, ConfigSpace.hyperparameters.IntegerHyperparameter):\n",
    "            domains.append(list(range(hp.lower, hp.upper + 1)))\n",
    "        else:\n",
    "            raise Exception(\"Unsupported type \" + str(type(hp)))\n",
    "    \n",
    "    # compute product\n",
    "    configs = []\n",
    "    for combo in it.product(*domains):\n",
    "        configs.append({name: combo[i] for i, name in enumerate(names)})\n",
    "    return configs\n",
    "\n",
    "def is_pipeline_forbidden(pl):\n",
    "    forbidden_combos = [\n",
    "        {\"feature-pre-processor\": sklearn.decomposition.FastICA, \"classifier\": sklearn.naive_bayes.MultinomialNB},\n",
    "        {\"feature-pre-processor\": sklearn.decomposition.PCA, \"classifier\": sklearn.naive_bayes.MultinomialNB},\n",
    "        {\"feature-pre-processor\": sklearn.decomposition.KernelPCA, \"classifier\": sklearn.naive_bayes.MultinomialNB},\n",
    "        {\"feature-pre-processor\": sklearn.kernel_approximation.RBFSampler, \"classifier\": sklearn.naive_bayes.MultinomialNB},\n",
    "        {\"feature-pre-processor\": sklearn.kernel_approximation.Nystroem, \"classifier\": sklearn.naive_bayes.MultinomialNB},\n",
    "        {\"data-pre-processor\": \n",
    "sklearn.preprocessing.PowerTransformer, \"classifier\": sklearn.naive_bayes.MultinomialNB},\n",
    "        {\"data-pre-processor\": sklearn.preprocessing.StandardScaler, \"classifier\": sklearn.naive_bayes.MultinomialNB},\n",
    "        {\"data-pre-processor\": sklearn.preprocessing.RobustScaler, \"classifier\": sklearn.naive_bayes.MultinomialNB}\n",
    "    ]\n",
    "    \n",
    "    representation = {}\n",
    "    for step_name, obj in pl.steps:\n",
    "        representation[step_name] = obj.__class__\n",
    "    \n",
    "    for combo in forbidden_combos:\n",
    "        matches = True\n",
    "        for key, val in combo.items():\n",
    "            if not key in representation or representation[key] != val:\n",
    "                matches = False\n",
    "                break\n",
    "        if matches:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "class PipelineSampler:\n",
    "    \n",
    "    def __init__(self, search_space_file, X, y, seed, dp_proba = 0, fp_proba = 0):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.dp_proba = dp_proba\n",
    "        self.fp_proba = fp_proba\n",
    "        self.search_space = []\n",
    "        self.search_space_description = json.load(open(search_space_file))\n",
    "        self.seed = np.random.randint(10**9) if seed is None else seed\n",
    "        self.rs = np.random.RandomState(self.seed)\n",
    "        \n",
    "        def get_factor_of_parameter_space(params):\n",
    "            factor_global = 1\n",
    "            for hp in params.get_hyperparameters():\n",
    "                factor_local = 1\n",
    "                if type(hp) == ConfigSpace.hyperparameters.CategoricalHyperparameter:\n",
    "                    factor_local = len(hp.choices)\n",
    "                elif type(hp) == ConfigSpace.hyperparameters.UniformFloatHyperparameter:\n",
    "                    factor_local = 10\n",
    "                elif type(hp) == ConfigSpace.hyperparameters.UniformIntegerHyperparameter:\n",
    "                    factor_local = min(10, hp.upper - hp.lower)\n",
    "\n",
    "                elif issubclass(hp.__class__, ConfigSpace.hyperparameters.NumericalHyperparameter):\n",
    "                    orders = np.log(hp.upper - hp.lower) / np.log(10)\n",
    "                    factor_local = orders\n",
    "                factor_global *= factor_local\n",
    "            return factor_global\n",
    "        \n",
    "        for step in self.search_space_description:\n",
    "            comp_descriptions = {}\n",
    "            for comp in step[\"components\"]:\n",
    "                params = config_json.read(comp[\"params\"])\n",
    "                params.random = self.rs\n",
    "                comp_descriptions[comp[\"class\"]] = {\"params\": params, \"weight\": get_factor_of_parameter_space(params)}\n",
    "            self.search_space.append(comp_descriptions)\n",
    "        \n",
    "    def sample_configured_algorithm(self, slot, do_build = True):\n",
    "        classes = list(self.search_space[slot].keys())\n",
    "        weights = np.log(1 + np.array([self.search_space[slot][clazz][\"weight\"] for clazz in classes]))\n",
    "        probabilities = weights / sum(weights)\n",
    "        target = self.rs.rand()\n",
    "        s = 0\n",
    "        for i, prob in enumerate(probabilities):\n",
    "            s += prob\n",
    "            if target <= s:\n",
    "                clazz = classes[i]\n",
    "                comp = [c for c in self.search_space_description[slot][\"components\"] if c[\"class\"] == clazz][0]\n",
    "                params = {}\n",
    "                config_space = self.search_space[slot][clazz][\"params\"]\n",
    "                sampled_config = config_space.sample_configuration(1)\n",
    "                for hp in config_space.get_hyperparameters():\n",
    "                    if hp.name in sampled_config:\n",
    "                        params[hp.name] = sampled_config[hp.name]\n",
    "                return build_estimator(comp, params, self.X, self.y) if do_build else (comp, params)\n",
    "    \n",
    "    ''' Samples a pipeline according to the weights\n",
    "    '''\n",
    "    def sample(self, do_build=True):\n",
    "        \n",
    "        steps = []\n",
    "        \n",
    "        # sample a data pre-processor?\n",
    "        if self.rs.rand() <= self.dp_proba:\n",
    "            steps.append((\"data-pre-processor\", self.sample_configured_algorithm(0, do_build)))\n",
    "            \n",
    "        # sample a feature pre-processor?\n",
    "        if self.rs.rand() <= self.dp_proba:\n",
    "            steps.append((\"feature-pre-processor\", self.sample_configured_algorithm(1, do_build)))\n",
    "        \n",
    "        # now sample a predictor\n",
    "        steps.append((\"predictor\", self.sample_configured_algorithm(2, do_build)))\n",
    "        return sklearn.pipeline.Pipeline(steps) if do_build else steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
