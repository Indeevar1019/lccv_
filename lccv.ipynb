{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import typing\n",
    "import logging\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats\n",
    "import time\n",
    "import sklearn.metrics\n",
    "import pynisher\n",
    "\n",
    "import inspect\n",
    "import traceback\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def format_learner(learner):\n",
    "    learner_name = str(learner).replace(\"\\n\", \" \").replace(\"\\t\", \" \")\n",
    "    for k in  range(20):\n",
    "        learner_name = learner_name.replace(\"  \", \" \")\n",
    "    return learner_name\n",
    "\n",
    "\n",
    "def _partition_train_test_data(\n",
    "        features: np.array, labels: np.array, n_test: int,\n",
    "        seed: int) -> typing.Tuple[np.array, np.array, np.array, np.array]:\n",
    "    \"\"\"\n",
    "    Partitions the dataset in a test set of the size of the requested size, and\n",
    "    a train set of size n_train.\n",
    "\n",
    "    :param features: The X-data\n",
    "    :param labels: The y-data\n",
    "    :param n_test: the requested test size\n",
    "    :param seed: The random seed\n",
    "    :return: A 4-tuple, consisting of the train features (2D np.array), the\n",
    "    train labels (1D np.array), the test features (2D np.array) and the test\n",
    "    labels (1D np.array)\n",
    "    \"\"\"\n",
    "    if seed is None:\n",
    "        raise ValueError('Seed can not be None (to ensure test set equality)')\n",
    "    np.random.seed(seed)\n",
    "    indices = np.arange(features.shape[0])\n",
    "    np.random.shuffle(indices)\n",
    "    features = features[indices]\n",
    "    labels = labels[indices]\n",
    "\n",
    "    return features[n_test:], labels[n_test:], features[:n_test], labels[:n_test]\n",
    "\n",
    "\n",
    "class EmpiricalLearningModel:\n",
    "    \n",
    "    def __init__(\n",
    "            self,\n",
    "            learner,\n",
    "            X,\n",
    "            y,\n",
    "            n_target,\n",
    "            seed,\n",
    "            fix_train_test_folds,\n",
    "            evaluator,\n",
    "            evaluator_kwargs,\n",
    "            base_scoring,\n",
    "            additional_scorings,\n",
    "            use_train_curve,\n",
    "            raise_errors\n",
    "    ):\n",
    "        \"\"\"\n",
    "\n",
    "        :param learner:\n",
    "        :param X:\n",
    "        :param y:\n",
    "        :param n_target:\n",
    "        :param seed:\n",
    "        :param fix_train_test_folds:\n",
    "        :param evaluator:\n",
    "        :param base_scoring: either a string (sklearn scorer) or a tuple `(name, descriptor)`,\n",
    "            where `name` is a string and `descriptor` is  either\n",
    "                (i) a scoring function, if it does not need to know the set of available labels or\n",
    "                (ii), it should be a dictionary with the arguments required by `make_scorer` except \"labels\",\n",
    "                    which will be filled by LCCV when building the scorer.\n",
    "        :param additional_scorings: iterable of scorings described in the same way as `base_scoring`\n",
    "        :param raise_errors: whether or not to raise errors (if not risen, they are logged via error)\n",
    "        \"\"\"\n",
    "        \n",
    "        # set up logger\n",
    "        self.logger = logging.getLogger('elm')\n",
    "        \n",
    "        self.learner = learner\n",
    "        self.active_seed = seed\n",
    "        self.fix_train_test_folds = fix_train_test_folds\n",
    "        self.use_train_curve = use_train_curve\n",
    "        self.raise_errors = raise_errors\n",
    "\n",
    "        # set evaluator and scoring\n",
    "        self.evaluator = evaluator if evaluator is not None else self.evaluate\n",
    "        if not callable(self.evaluator):\n",
    "            raise Exception(f\"Evaluator is of type {type(self.evaluator)}, which is not a callable.\")\n",
    "        self.evaluator_kwargs = evaluator_kwargs\n",
    "\n",
    "        # set scoring functions\n",
    "        self.base_scoring = base_scoring\n",
    "        self.additional_scorings = list(additional_scorings)\n",
    "\n",
    "        # the data is only used if no evaluator is given\n",
    "        if evaluator is None:\n",
    "            \n",
    "            if X.shape[0] <= 0:\n",
    "                raise Exception(f\"Recieved dataset with non-positive number of instances. Shape is {X.shape}\")\n",
    "            \n",
    "            n_test = X.shape[0] - n_target # portion of data that exceeds the target value is used for testing\n",
    "            \n",
    "            if fix_train_test_folds:\n",
    "                self.X_train, self.y_train, self.X_test, self.y_test = _partition_train_test_data(X, y, n_test, seed)\n",
    "                self.logger.info(f\"Train labels: \\n{self.y_train}\")\n",
    "                self.logger.info(f\"Test labels: \\n{self.y_test}\")\n",
    "            else:\n",
    "                self.X = X\n",
    "                self.y = y\n",
    "                self.n_test = n_test\n",
    "\n",
    "        \"\"\"\n",
    "        if a scoring function is given as a string, the existing labels are added through make_scorer.\n",
    "        this is a work-around since sklearn does not allow to provide the labels when getting a scoring with get_scorer\n",
    "        it is also necessary here and NOT in the constructor, because the labels must be the ones used in the training set.\n",
    "        \"\"\"\n",
    "        for i, scoring in enumerate([self.base_scoring] + self.additional_scorings):\n",
    "            if type(scoring) == str:\n",
    "                tmp_scorer = sklearn.metrics.get_scorer(scoring)\n",
    "                needs_labels = \"labels\" in inspect.signature(tmp_scorer._score_func).parameters\n",
    "                kws = {\n",
    "                    \"score_func\": tmp_scorer._score_func,\n",
    "                    \"greater_is_better\": tmp_scorer._sign == 1,\n",
    "                    \"needs_proba\": type(tmp_scorer) == sklearn.metrics._scorer._ProbaScorer,\n",
    "                    \"needs_threshold\": type(tmp_scorer) == sklearn.metrics._scorer._ThresholdScorer,\n",
    "                }\n",
    "                if needs_labels:\n",
    "                    kws[\"labels\"] = list(np.unique(y))\n",
    "                scoring = (scoring, sklearn.metrics.make_scorer(**kws))\n",
    "            elif type(scoring) != tuple:\n",
    "                raise ValueError(\n",
    "                    f\"{'base_scoring' if i == 0 else f'The {i-1}th additional scoring'}\"\n",
    "                    f\"is of type {type(scoring)} but must be a string or a tuple of size 2.\"\n",
    "                )\n",
    "            elif len(scoring) != 2:\n",
    "                raise ValueError(\n",
    "                    f\"{'base_scoring' if i == 0 else f'The {i - 1}th additional scoring'}\"\n",
    "                    f\"has length {len(scoring)} but should have length 2.\"\n",
    "                )\n",
    "            elif type(scoring[0]) != str:\n",
    "                raise ValueError(\n",
    "                    f\"{'base_scoring' if i == 0 else f'The {i - 1}th additional scoring'}\"\n",
    "                    f\"requires a str in the first field for the name but has {type(scoring[0])}.\"\n",
    "                )\n",
    "            elif not callable(scoring[1]):\n",
    "                raise ValueError(\n",
    "                    f\"Scoring is of type {type(self.scoring)}, which is not a callable.\"\n",
    "                    \"Make sure to pass a string or Callable.\"\n",
    "                )\n",
    "            if i == 0:\n",
    "                self.base_scoring = scoring\n",
    "            else:\n",
    "                self.additional_scorings[i - 1] = scoring\n",
    "\n",
    "        columns = [\"anchor\", \"seed\", \"fittime\"]\n",
    "        for scoring, _ in [self.base_scoring] + self.additional_scorings:\n",
    "            columns.extend([\n",
    "                f\"scoretime_train_{scoring}\",\n",
    "                f\"score_train_{scoring}\",\n",
    "                f\"scoretime_test_{scoring}\",\n",
    "                f\"score_test_{scoring}\"\n",
    "            ])\n",
    "\n",
    "        # initialize data\n",
    "        self.df = pd.DataFrame([], columns=columns)\n",
    "        self.rs = np.random.RandomState(seed)\n",
    "\n",
    "    def evaluate(self, learner_inst, anchor, timeout, base_scoring, additional_scorings):\n",
    "\n",
    "        self.active_seed += 1\n",
    "        self.logger.debug(\"Computing training data\")\n",
    "        \n",
    "        # obtain train and test data (depending on configuration)\n",
    "        if self.fix_train_test_folds:\n",
    "            self.logger.info(\"Re-using pre-defined train and test folds\")\n",
    "            X_train, y_train, X_test, y_test = self.X_train, self.y_train, self.X_test, self.y_test\n",
    "        else:\n",
    "            X_train, y_train, X_test, y_test = _partition_train_test_data(self.X, self.y, self.n_test, self.active_seed)\n",
    "            self.logger.info(f\"Dynamically creating a train and test fold with seed {self.active_seed}.\")\n",
    "        \n",
    "        indices = self.rs.choice(X_train.shape[0], anchor, replace=False)\n",
    "        X_train = X_train[indices]\n",
    "        y_train = y_train[indices]\n",
    "        self.logger.debug(f\"Created train portion. Labels in train/test data: {len(np.unique(y_train))}/{len(np.unique(y_test))}\")\n",
    "        \n",
    "        hash_before = hash(X_train.tobytes())\n",
    "        learner_inst = sklearn.base.clone(learner_inst)\n",
    "        self.logger.info(f\"Training {format_learner(learner_inst)} on data of shape {X_train.shape}. Timeout is {timeout}\")\n",
    "        start_eval = time.time()\n",
    "        if timeout is None:\n",
    "            learner_inst.fit(X_train, y_train)\n",
    "        elif timeout > 1:\n",
    "            with pynisher.limit(func=learner_inst.fit, wall_time=timeout) as executor:\n",
    "                learner_inst = executor(X_train, y_train)\n",
    "        else:\n",
    "            raise pynisher.WallTimeoutException()\n",
    "\n",
    "        end = time.time()\n",
    "        self.logger.debug(f\"Training ready after {int((end - start_eval) * 1000)}ms. Now obtaining predictions.\")\n",
    "        results = {\n",
    "            \"fittime\": end - start_eval\n",
    "        }\n",
    "        for scoring_name, scoring_fun in [base_scoring] + additional_scorings:\n",
    "            start = time.time()\n",
    "            try:\n",
    "                if timeout is None:\n",
    "                    score_test = scoring_fun(learner_inst, X_test, y_test)\n",
    "                elif timeout > start - start_eval + 1:\n",
    "                    with pynisher.limit(func=scoring_fun, wall_time=timeout - (start - start_eval)) as executor:\n",
    "                        score_test = executor(learner_inst, X_test, y_test)\n",
    "                else:\n",
    "                    raise pynisher.WallTimeoutException()\n",
    "            except KeyboardInterrupt:\n",
    "                raise\n",
    "            except pynisher.WallTimeoutException:\n",
    "                raise\n",
    "            except Exception as e:\n",
    "                if self.raise_errors:\n",
    "                    raise\n",
    "                else:\n",
    "                    self.logger.error(f\"{traceback.format_exc()}\")\n",
    "                    score_test = np.nan\n",
    "            runtime_test = time.time() - start\n",
    "\n",
    "            if self.use_train_curve:\n",
    "                start = time.time()\n",
    "                try:\n",
    "                    if timeout is None:\n",
    "                        score_train = scoring_fun(learner_inst, X_train, y_train)\n",
    "                    elif timeout > start - start_eval + 1:\n",
    "                        with pynisher.limit(func=scoring_fun, wall_time=timeout - (start - start_eval)) as executor:\n",
    "                            score_train = executor(learner_inst, X_train, y_train)\n",
    "                    else:\n",
    "                        raise pynisher.WallTimeoutException()\n",
    "                except KeyboardInterrupt:\n",
    "                    raise\n",
    "                except pynisher.WallTimeoutException:\n",
    "                    raise\n",
    "                except Exception as e:\n",
    "                    if self.raise_errors:\n",
    "                        raise\n",
    "                    else:\n",
    "                        self.logger.error(f\"{traceback.format_exc()}\")\n",
    "                        score_train = np.nan\n",
    "                runtime_train = time.time() - start\n",
    "            else:\n",
    "                score_train, runtime_train = np.nan, 0\n",
    "            results.update({\n",
    "                f\"scoretime_train_{scoring_name}\": runtime_train,\n",
    "                f\"score_train_{scoring_name}\": score_train,\n",
    "                f\"scoretime_test_{scoring_name}\": runtime_test,\n",
    "                f\"score_test_{scoring_name}\": score_test\n",
    "            })\n",
    "        end = time.time()\n",
    "        self.logger.info(\n",
    "            f\"Evaluation ready after {int((end - start) * 1000)}ms.\"\n",
    "            \"Score of model on {y_test.shape[0]} validation/test instances is {score_test}.\"\n",
    "        )\n",
    "        hash_after = hash(X_train.tobytes())\n",
    "        if hash_before != hash_after:\n",
    "            raise Exception(\n",
    "                \"Evaluation of pipeline has changed the data.\"\n",
    "                \"Please make sure to evaluate pipelines that do not change the data in place.\"\n",
    "            )\n",
    "        return results\n",
    "    \n",
    "    def compute_and_add_sample(self, anchor, seed=None, timeout=None, verbose=False):\n",
    "        evaluation_result = self.evaluator(\n",
    "            self.learner,\n",
    "            anchor,\n",
    "            timeout / 1000 if timeout is not None else None,\n",
    "            self.base_scoring,\n",
    "            self.additional_scorings,\n",
    "            **self.evaluator_kwargs\n",
    "        )\n",
    "\n",
    "        row = [\n",
    "            anchor,\n",
    "            seed,\n",
    "            evaluation_result[\"fittime\"]\n",
    "        ]\n",
    "        for scoring_name, _ in [self.base_scoring] + self.additional_scorings:\n",
    "            row.extend([\n",
    "                evaluation_result[f\"scoretime_train_{scoring_name}\"],\n",
    "                evaluation_result[f\"score_train_{scoring_name}\"],\n",
    "                evaluation_result[f\"scoretime_test_{scoring_name}\"],\n",
    "                evaluation_result[f\"score_test_{scoring_name}\"]\n",
    "            ])\n",
    "        self.logger.debug(f\"Sample value computed. Extending database.\")\n",
    "        self.df.loc[len(self.df)] = row\n",
    "        self.df = self.df.astype({\"anchor\": int, \"seed\": int})\n",
    "        scoring = self.base_scoring[0]\n",
    "        return evaluation_result[f\"score_train_{scoring}\"], evaluation_result[f\"score_test_{scoring}\"]\n",
    "    \n",
    "    def get_values_at_anchor(self, anchor, scoring=None, test_scores = True):\n",
    "        if scoring is None:\n",
    "            scoring = self.base_scoring[0]\n",
    "        col = \"score_\" + (\"test\" if test_scores else \"train\") + \"_\" + scoring\n",
    "        return self.df[self.df[\"anchor\"] == anchor][col].values\n",
    "    \n",
    "    def get_best_worst_train_score(self, scoring = None):\n",
    "        if scoring is None:\n",
    "            scoring = self.base_scoring[0]\n",
    "        return max([min(g) for i, g in self.df.groupby(\"anchor\")[f\"score_train_{scoring}\"]])\n",
    "    \n",
    "    def get_mean_performance_at_anchor(self, anchor, scoring=None, test_scores=True):\n",
    "        return np.mean(self.get_values_at_anchor(anchor, scoring=scoring, test_scores=test_scores))\n",
    "    \n",
    "    def get_mean_curve(self, test_scores = True):\n",
    "        anchors = sorted(pd.unique(self.df[\"anchor\"]))\n",
    "        return anchors, [self.get_mean_performance_at_anchor(a, test_scores = test_scores) for a in anchors]\n",
    "    \n",
    "    def get_runtimes_at_anchor(self, anchor):\n",
    "        return self.df[self.df[\"anchor\"] == anchor][[c for c in self.df.columns if \"time\" in c]]\n",
    "    \n",
    "    def get_conf_interval_size_at_target(self, target):\n",
    "        if len (self.df[self.df[\"anchor\"] == target]) == 0:\n",
    "            return 1\n",
    "        ci = self.get_normal_estimates(anchor = target)[\"conf\"]\n",
    "        return ci[1] - ci[0]\n",
    "    \n",
    "    def get_lc_estimate_at_target(self, target):\n",
    "        return self.get_mmf()[1](target)\n",
    "    \n",
    "    def get_normal_estimates(self, anchor = None, round_precision=100, scoring=None, validation = True):\n",
    "\n",
    "        if anchor is None:\n",
    "            anchors = sorted(np.unique(self.df[\"anchor\"]))\n",
    "            out = {}\n",
    "            for anchor in anchors:\n",
    "                out[int(anchor)] = self.get_normal_estimates(anchor)\n",
    "            return out\n",
    "\n",
    "        if scoring is None:\n",
    "            scoring = self.base_scoring[0]\n",
    "\n",
    "        dfProbesAtAnchor = self.df[self.df[\"anchor\"] == anchor]\n",
    "        mu = np.mean(dfProbesAtAnchor[\"score_\" + (\"test\" if validation else \"train\") + \"_\" + scoring])\n",
    "        sigma = np.std(dfProbesAtAnchor[\"score_\" + (\"test\" if validation else \"train\") + \"_\" + scoring])\n",
    "        return {\n",
    "            \"n\": len(dfProbesAtAnchor[\"score_\" + (\"test\" if validation else \"train\") + \"_\" + scoring]),\n",
    "            \"mean\": np.round(mu, round_precision),\n",
    "            \"std\": np.round(sigma, round_precision),\n",
    "            \"conf\": np.round(scipy.stats.norm.interval(0.95, loc=mu, scale=sigma/np.sqrt(len(dfProbesAtAnchor))) if sigma > 0 else (mu, mu), round_precision)\n",
    "        }\n",
    "    \n",
    "    def get_slope_ranges(self):\n",
    "        est = self.get_normal_estimates()\n",
    "        anchors = [s for s in est]\n",
    "        ranges = []\n",
    "        for i, anchor in enumerate(anchors):\n",
    "            if i > 0:\n",
    "                anchor_prev_last = anchors[i - 1]\n",
    "                anchor_last = anchors[i]\n",
    "                \n",
    "                # compute confidence bounds of prev last and last anchor\n",
    "                if est[anchor_prev_last][\"n\"] > 1:\n",
    "                    lower_prev_last = est[anchor_prev_last][\"conf\"][0]\n",
    "                    upper_prev_last = est[anchor_prev_last][\"conf\"][1]\n",
    "                else:\n",
    "                    lower_prev_last = upper_prev_last = est[anchor_prev_last][\"mean\"]\n",
    "                if est[anchor_last][\"n\"] > 1:\n",
    "                    lower_last = est[anchor_last][\"conf\"][0]\n",
    "                    upper_last = est[anchor_last][\"conf\"][1]\n",
    "                else:\n",
    "                    lower_last = upper_last = est[anchor_last][\"mean\"]\n",
    "                \n",
    "                # compute slope range\n",
    "                pessimistic_slope = max(0, (lower_last - upper_prev_last) / (anchor_last - anchor_prev_last))\n",
    "                optimistic_slope = max(0, (upper_last - lower_prev_last) / (anchor_last - anchor_prev_last))\n",
    "                ranges.append((pessimistic_slope, optimistic_slope))\n",
    "        return ranges\n",
    "    \n",
    "    def get_slope_range_in_last_segment(self):\n",
    "        return self.get_slope_ranges()[-1]\n",
    "    \n",
    "    def get_performance_interval_at_target(self, target):\n",
    "        pessimistic_slope, optimistic_slope = self.get_slope_range_in_last_segment()\n",
    "        anchors = sorted(np.unique(self.df[\"anchor\"]))\n",
    "        last_anchor = anchors[-1]\n",
    "        normal_estimates = self.get_normal_estimates()[last_anchor]\n",
    "        if normal_estimates[\"n\"] > 1:\n",
    "            last_conf = normal_estimates[\"conf\"]\n",
    "            if normal_estimates[\"std\"] > 0:\n",
    "                last_conf_lower = last_conf[0]\n",
    "                last_conf_upper = last_conf[1]\n",
    "            else:\n",
    "                last_conf_lower = last_conf_upper = normal_estimates[\"mean\"]\n",
    "                last_conf = (last_conf_lower, last_conf_upper)\n",
    "        else:\n",
    "            last_conf_lower = last_conf_upper = normal_estimates[\"mean\"]\n",
    "            last_conf = (last_conf_lower, last_conf_upper)\n",
    "        if any(np.isnan(last_conf)):\n",
    "            return last_conf\n",
    "        if np.isnan(optimistic_slope):\n",
    "            raise Exception(\"Slope must not be nan\")\n",
    "        return pessimistic_slope * (target - last_anchor) + last_conf_lower, optimistic_slope * (target - last_anchor) + last_conf_upper\n",
    "        \n",
    "    def get_ipl(self):\n",
    "        anchors = sorted(list(pd.unique(self.df[\"anchor\"])))\n",
    "        scores = [np.mean(self.df[self.df[\"anchor\"] == a][f\"{self.base_scoring[0]}_val\"]) for a in anchors]\n",
    "\n",
    "        def ipl(beta):\n",
    "            a, b, c = tuple(beta.astype(float))\n",
    "            pl = lambda x: a + b * x **(-c)\n",
    "            penalty = []\n",
    "            for i, anchor in enumerate(anchors):\n",
    "                penalty.append((pl(anchor) - scores[i])**2)\n",
    "            return np.array(penalty)\n",
    "\n",
    "        a, b, c = tuple(scipy.optimize.least_squares(ipl, np.array([1, 1, 1]), method=\"lm\").x)\n",
    "        return lambda x: a + b * x **(-c)\n",
    "    \n",
    "    def get_mmf(self, validation_curve=True, scoring=None):\n",
    "        anchors = sorted(list(pd.unique(self.df[\"anchor\"])))\n",
    "        if scoring is None:\n",
    "            scoring = self.base_scoring[0]\n",
    "        scores = [\n",
    "            np.mean(self.df[self.df[\"anchor\"] == a][\"score_\" + (\"test\" if validation_curve else \"train\") + \"_\" + scoring])\n",
    "            for a in anchors\n",
    "        ]\n",
    "        weights = [2**i for i in range(len(anchors))]\n",
    "        def mmf(beta):\n",
    "            a, b, c, d = tuple(beta.astype(float))\n",
    "            fun = lambda x: (a * b + c * x ** d)/(b + x ** d)\n",
    "            penalties = []\n",
    "            for i, anchor in enumerate(anchors):\n",
    "                penalty = weights[i]  * ((scores[i] - fun(anchor)) ** 2) # give more weights on higher anchors\n",
    "                penalties.append(penalty if not np.isnan(penalty) else 10**6)\n",
    "            return sum(penalties)\n",
    "        \n",
    "        factor = 1 if validation_curve else -1\n",
    "        const = {\n",
    "            \"type\": \"ineq\", \"fun\": lambda x: -factor * x[1] * (x[2]-x[0])*x[3]\n",
    "        }\n",
    "\n",
    "        a, b, c, d = tuple(scipy.optimize.minimize(mmf, np.array([0.5,1,1,-1]), constraints=const).x)\n",
    "        return (a, b, c, d), lambda x: (a * b + c * x ** d)/(b + x ** d)\n",
    "    \n",
    "    def predict_runtime(self, target_anchor):\n",
    "        lr = sklearn.linear_model.LinearRegression()\n",
    "        X = self.df[[\"anchor\"]].values\n",
    "        X = np.row_stack([X, [[0]]])\n",
    "        X = np.column_stack([X, X[:]**2])\n",
    "        y = self.df[\"runtime\"].values\n",
    "        y = np.append(y, [0])\n",
    "        lr.fit(X, y)\n",
    "        b = np.abs(lr.coef_[0])\n",
    "        a = np.abs(lr.coef_[1])\n",
    "        return a * (target_anchor**2) + b * target_anchor + lr.intercept_\n",
    "    \n",
    "    def get_max_size_for_runtime(self, runtime):\n",
    "        lr = sklearn.linear_model.LinearRegression()\n",
    "        X = self.df[[\"anchor\"]].values\n",
    "        X = np.row_stack([X, [[0]]])\n",
    "        X = np.column_stack([X, X[:]**2])\n",
    "        y = self.df[\"runtime\"].values\n",
    "        y = np.append(y, [0])\n",
    "        lr.fit(X, y)\n",
    "        b = np.abs(lr.coef_[0])\n",
    "        a = np.abs(lr.coef_[1])\n",
    "        inner = (-b/(2 * a))**2 - (lr.intercept_ - runtime) / a\n",
    "        return -b/(2 * a) + np.sqrt(inner)\n",
    "    \n",
    "    def visualize(self, max_anchor = 1000, r = None):\n",
    "        anchors = sorted(list(pd.unique(self.df[\"anchor\"])))\n",
    "        scores_train = [self.get_normal_estimates(a, validation=False) for a in anchors]\n",
    "        scores_valid = [self.get_normal_estimates(a, validation=True) for a in anchors]\n",
    "        lc_train_params, lc_train = self.get_mmf(False)\n",
    "        lc_test_params, lc_valid = self.get_mmf(True)\n",
    "        \n",
    "        fig, ax = plt.subplots()\n",
    "        ax.scatter(anchors, [e[\"mean\"] for e in scores_train])\n",
    "        ax.scatter(anchors, [e[\"mean\"] for e in scores_valid])\n",
    "        domain = np.linspace(64, max_anchor, 100)\n",
    "        ax.plot(domain, lc_train(domain), color=\"C0\")\n",
    "        ax.fill_between(anchors, [v[\"mean\"] - v[\"std\"] for v in scores_train], [v[\"mean\"] + v[\"std\"] for v in scores_train], alpha=0.2, color=\"C0\")\n",
    "        ax.plot(domain, lc_valid(domain), color=\"C1\")\n",
    "        ax.fill_between(anchors, [v[\"mean\"] - v[\"std\"] for v in scores_valid], [v[\"mean\"] + v[\"std\"] for v in scores_valid], alpha = 0.2, color=\"C1\")\n",
    "        \n",
    "        # create lines that project based on convexity\n",
    "        val_at_target_pessimistic, val_at_target_optimistic = self.get_performance_interval_at_target(max_anchor)\n",
    "        ax.plot([anchors[-2], max_anchor], [scores_valid[-2][\"mean\"] + scores_valid[-2][\"std\"], val_at_target_pessimistic], color=\"C3\", linestyle=\"--\")\n",
    "        ax.plot([anchors[-2], max_anchor], [scores_valid[-2][\"mean\"] - scores_valid[-2][\"std\"], val_at_target_optimistic], color=\"C2\", linestyle=\"--\")\n",
    "        \n",
    "        if r is not None:\n",
    "            ax.axhline(r, color=\"black\", linestyle=\"--\")\n",
    "        plt.show()\n",
    "    \n",
    "\n",
    "def lccv(\n",
    "        learner_inst,\n",
    "        X,\n",
    "        y,\n",
    "        r,\n",
    "        timeout=None,\n",
    "        base=2,\n",
    "        min_exp=6,\n",
    "        MAX_ESTIMATE_MARGIN_FOR_FULL_EVALUATION=0.005,\n",
    "        MAX_EVALUATIONS=10,\n",
    "        target_anchor=.9,\n",
    "        schedule=None,\n",
    "        return_estimate_on_incomplete_runs=False,\n",
    "        max_conf_interval_size_default=0.1,\n",
    "        max_conf_interval_size_target=0.001,\n",
    "        enforce_all_anchor_evaluations=False,\n",
    "        seed=0,\n",
    "        verbose=False,\n",
    "        logger=None,\n",
    "        min_evals_for_stability=3,\n",
    "        use_train_curve=True,\n",
    "        fix_train_test_folds=False,\n",
    "        evaluator=None,\n",
    "        evaluator_kwargs={},\n",
    "        base_scoring=\"accuracy\",\n",
    "        additional_scorings=[],\n",
    "        visualize_lcs = False,\n",
    "        raise_exceptions = True\n",
    "):\n",
    "    \"\"\"\n",
    "    Evaluates a learner in an iterative fashion, using learning curves. The\n",
    "    method builds upon the assumption that learning curves are convex. After\n",
    "    each iteration, it checks whether the convexity assumption is still valid.\n",
    "    If not, it tries to repair it.\n",
    "    Also, after each iteration it checks whether the performance of the best\n",
    "    seen learner so far is still reachable by making an optimistic extrapolation.\n",
    "    If not, it stops the evaluation.\n",
    "\n",
    "    :param learner_inst: The learner to be evaluated\n",
    "    :param X: The features on which the learner needs to be evaluated\n",
    "    :param y: The labels on which the learner needs to be trained\n",
    "    :param r: The best seen performance so far (lower is better). Fill in 0.0 if\n",
    "    no learners have been evaluated prior to the learner.\n",
    "    :param timeout: The maximal runtime for this specific leaner. Fill in None\n",
    "    to avoid cutting of the evaluation.\n",
    "    :param base: The base factor to increase the sample sizes of the learning\n",
    "    curve.\n",
    "    :param min_exp: The first exponent of the learning curve.\n",
    "    :param MAX_ESTIMATE_MARGIN_FOR_FULL_EVALUATION: The maximum number of\n",
    "    evaluations to be performed\n",
    "    :param MAX_EVALUATIONS:\n",
    "    :param target_anchor:\n",
    "    :param schedule: define the anchors for which scores should be computed\n",
    "    :param return_estimate_on_incomplete_runs:\n",
    "    :param max_conf_interval_size_default:\n",
    "    :param max_conf_interval_size_target:\n",
    "    :param enforce_all_anchor_evaluations:\n",
    "    :param seed:\n",
    "    :param verbose:\n",
    "    :param logger:\n",
    "    :param min_evals_for_stability:\n",
    "    :param use_train_curve: If True, then the evaluation stops as soon as the train curve drops under the threshold r\n",
    "    :param evaluator: Function to be used to query a noisy score at some anchor. To be maximized!\n",
    "    :param evaluator_kwargs: arguments to be forwarded to the evaluator function\n",
    "    :param base_scoring: either a string (sklearn scorer) or a tuple `(name, descriptor)`,\n",
    "            where `name` is a string and `descriptor` is  either\n",
    "                (i) a scoring function, if it does not need to know the set of available labels or\n",
    "                (ii), it should be a dictionary with the arguments required by `make_scorer` except \"labels\",\n",
    "                    which will be filled by LCCV when building the scorer.\n",
    "        :param additional_scorings: iterable of scorings described in the same way as `base_scoring`\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    # create standard logger if none is given\n",
    "    if logger is None:\n",
    "        logger = logging.getLogger('lccv')\n",
    "    logger.debug(\"timeout = \" + str(timeout) + \", \" +\n",
    "                 \"BASE = \" + str(base) + \", \" +\n",
    "                 \"min_exp = \" + str(min_exp) + \", \" +\n",
    "                 \"MAX_ESTIMATE_MARGIN_FOR_FULL_EVALUATION = \" + str(MAX_ESTIMATE_MARGIN_FOR_FULL_EVALUATION) + \", \" +\n",
    "                 \"MAX_EVALUATIONS = \" + str(MAX_EVALUATIONS) + \", \" +\n",
    "                 \"target_anchor = \" + str(target_anchor) + \", \" +\n",
    "                 \"return_estimate_on_incomplete_runs = \" + str(return_estimate_on_incomplete_runs) + \", \" +\n",
    "                 \"max_conf_interval_size_default = \" + str(max_conf_interval_size_default) + \", \" +\n",
    "                 \"max_conf_interval_size_target = \" + str(max_conf_interval_size_target) +  \", \" +\n",
    "                 \"enforce_all_anchor_evaluations = \" + str(enforce_all_anchor_evaluations) +  \", \" +\n",
    "                 \"seed = \" + str(seed) +  \", \" +\n",
    "                 \"min_evals_for_stability = \" + str(min_evals_for_stability) + \", \" +\n",
    "                 \"fix_train_test_folds = \" + str(fix_train_test_folds))\n",
    "    # intialize\n",
    "    tic = time.time()\n",
    "    deadline = tic + timeout if timeout is not None else None\n",
    "\n",
    "    # configure the exponents and status variables\n",
    "    if target_anchor < 1:\n",
    "        if X is None:\n",
    "            raise Exception(\n",
    "                \"If no data is given, the `target_anchor` parameter must be specified as a positive integer.\"\n",
    "            )\n",
    "        target_anchor = int(np.floor(X.shape[0] * target_anchor))\n",
    "    \n",
    "    # initialize important variables and datastructures\n",
    "    max_exp = np.log(target_anchor) / np.log(base)\n",
    "    if schedule is None:\n",
    "        schedule = [base**i for i in list(range(min_exp, int(np.ceil(max_exp))))] + [target_anchor]\n",
    "    elif any(np.argsort(schedule) != list(range(len(schedule)))):\n",
    "        raise ValueError(\"parameter `schedule` must be sorted\")\n",
    "    slopes = (len(schedule) - 1) * [np.nan]\n",
    "    elm = EmpiricalLearningModel(\n",
    "        learner=learner_inst,\n",
    "        X=X,\n",
    "        y=y,\n",
    "        n_target=target_anchor,\n",
    "        seed=seed,\n",
    "        fix_train_test_folds=fix_train_test_folds,\n",
    "        evaluator=evaluator,\n",
    "        evaluator_kwargs=evaluator_kwargs,\n",
    "        base_scoring=base_scoring,\n",
    "        additional_scorings=additional_scorings,\n",
    "        use_train_curve=use_train_curve,\n",
    "        raise_errors=raise_exceptions\n",
    "    )\n",
    "    T = len(schedule) - 1\n",
    "    t = 0 if r < np.inf or enforce_all_anchor_evaluations else T\n",
    "    repair_convexity = False\n",
    "    \n",
    "    # announce start event together with state variable values\n",
    "    logger.info(f\"\"\"Running LCCV {'on ' + str(X.shape) + '-shaped data' if X is not None else 'with custom evaluator.'}. Overview:\n",
    "    learner: {format_learner(learner_inst)}\n",
    "    r: {r}\n",
    "    min_exp: {min_exp}\n",
    "    max_exp: {max_exp}\n",
    "    Seed is {seed}\n",
    "    t_0: {t}\n",
    "    Schedule: {schedule}\"\"\")\n",
    "    \n",
    "    # MAIN LOOP\n",
    "    while t <= T and elm.get_conf_interval_size_at_target(target_anchor) > max_conf_interval_size_target and len(elm.get_values_at_anchor(target_anchor)) < MAX_EVALUATIONS:\n",
    "        \n",
    "        remaining_time = deadline - time.time() - 0.1 if deadline is not None else np.inf\n",
    "        if remaining_time < 1:\n",
    "            logger.info(\"Timeout observed, stopping outer loop of LCCV\")\n",
    "            break\n",
    "        \n",
    "        # initialize stage-specific variables\n",
    "        eps = max_conf_interval_size_target if t == T else max_conf_interval_size_default\n",
    "        s_t = schedule[t]\n",
    "        num_evaluations_at_t = len(elm.get_values_at_anchor(s_t))\n",
    "        logger.info(f\"Running iteration for t = {t}. Anchor point s_t is {s_t}. Remaining time: {remaining_time}s\")\n",
    "        \n",
    "        # INNER LOOP: acquire observations at anchor until stability is reached, or just a single one to repair convexity\n",
    "        while repair_convexity or num_evaluations_at_t < min_evals_for_stability or (elm.get_conf_interval_size_at_target(s_t) > eps and num_evaluations_at_t < MAX_EVALUATIONS):\n",
    "            \n",
    "            remaining_time = deadline - time.time() - 0.1 if deadline is not None else np.inf\n",
    "            if remaining_time < 1:\n",
    "                logger.info(\"Timeout observed, stopping inner loop of LCCV\")\n",
    "                break\n",
    "            \n",
    "            # unset flag for convexity repair\n",
    "            repair_convexity = False\n",
    "            \n",
    "            # compute next sample\n",
    "            try:\n",
    "                seed_used = 13 * (1 + seed) + num_evaluations_at_t\n",
    "                logger.debug(f\"Adding point at anchor {s_t} with seed is {seed_used}. Remaining time: {remaining_time}s\")\n",
    "                score_train, score_test = elm.compute_and_add_sample(s_t, seed_used, (deadline - time.time() - 0.1) * 1000 if deadline is not None else None, verbose=verbose)\n",
    "                num_evaluations_at_t += 1\n",
    "                logger.debug(f\"Sample computed successfully. Observed performance was {np.round(score_train, 4)} (train) and {np.round(score_test, 4)} (test).\")\n",
    "            except pynisher.WallTimeoutException:\n",
    "                timeouted = True\n",
    "                logger.info(\"Observed timeout. Stopping LCCV.\")\n",
    "                break\n",
    "            except Exception:\n",
    "                if raise_exceptions:\n",
    "                    raise\n",
    "                logger.warning(\n",
    "                    f\"Observed an exception at anchor {s_t}.\"\n",
    "                    \"Set raise_exceptions=True if you want this to cause the evaluation to fail.\"\n",
    "                    f\"Trace: {traceback.format_exc()}\"\n",
    "                )\n",
    "                score_train, score_test = np.nan, np.nan\n",
    "                num_evaluations_at_t += 1\n",
    "            \n",
    "            # check wheter a repair is needed\n",
    "            if num_evaluations_at_t >= min_evals_for_stability and t < T and t > 2:                    \n",
    "                slopes = elm.get_slope_ranges()\n",
    "                if len(slopes) < 2:\n",
    "                    raise Exception(f\"There should be two slope ranges for t > 2 (t is {t}), but we observed only 1.\")\n",
    "                if slopes[t - 2] > slopes[t - 1] and len(elm.get_values_at_anchor(schedule[t - 1])) < MAX_EVALUATIONS:\n",
    "                    repair_convexity = True\n",
    "                    break\n",
    "\n",
    "        # check training curve\n",
    "        if use_train_curve != False:\n",
    "            \n",
    "            check_training_curve = (type(use_train_curve) == bool) or (callable(use_train_curve) and use_train_curve(learner_inst, s_t))\n",
    "            \n",
    "            if check_training_curve and elm.get_best_worst_train_score() < r:\n",
    "                logger.info(f\"Train curve has value {elm.get_best_worst_train_score()} that is already worse than r = {r}. Stopping.\")\n",
    "                break\n",
    "        \n",
    "        # after the last stage, we dont need any more tests\n",
    "        if t == T:\n",
    "            logger.info(\"Last iteration has been finished. Not testing anything else anymore.\")\n",
    "            break\n",
    "        \n",
    "        # now decide how to proceed\n",
    "        if repair_convexity:\n",
    "            t -= 1\n",
    "            logger.debug(f\"Convexity needs to be repaired, stepping back. t is now {t}\")\n",
    "        elif t >= 2 and elm.get_performance_interval_at_target(target_anchor)[1] < r:\n",
    "            \n",
    "            if visualize_lcs:\n",
    "                logger.debug(f\"Visualizing curve\")\n",
    "                elm.visualize(schedule[-1], r)\n",
    "            \n",
    "            estimate_for_target_performance = elm.get_performance_interval_at_target(target_anchor)\n",
    "            optimistic_estimate_for_target_performance = estimate_for_target_performance[1]\n",
    "            \n",
    "            # prepare data for cut-off summary\n",
    "            pessimistic_slope, optimistic_slope = elm.get_slope_range_in_last_segment()\n",
    "            estimates = elm.get_normal_estimates()\n",
    "            anchors = sorted(np.unique(elm.df[\"anchor\"]))\n",
    "            i = -1\n",
    "            if min_evals_for_stability > 1:\n",
    "                while len(elm.df[elm.df[\"anchor\"] == anchors[i]]) < 2:\n",
    "                    i -= 1\n",
    "            last_anchor = s_t\n",
    "            normal_estimates_last = estimates[last_anchor]\n",
    "            last_conf = normal_estimates_last[\"conf\"]\n",
    "            \n",
    "            # inform about cut-off\n",
    "            logger.info(f\"Impossibly reachable. Best possible score by bound is {optimistic_estimate_for_target_performance}. Stopping after anchor s_t = {s_t} and returning nan.\")\n",
    "            logger.debug(f\"\"\"Details about stop:\n",
    "            Data:\n",
    "            {elm.df}\n",
    "            Normal Estimates: \"\"\" + ''.join([\"\\n\\t\\t\" + str(s_t) + \": \" + (str(estimates[s_t]) if s_t in estimates else \"n/a\") for s_t in schedule]) + \"\\n\\tSlope Ranges:\" + ''.join([\"\\n\\t\\t\" + str(schedule[i]) + \" - \" + str(schedule[i + 1]) + \": \" +  str(e) for i, e in enumerate(elm.get_slope_ranges())]) + f\"\"\"\n",
    "            Last anchor: {last_anchor}\n",
    "            Optimistic offset at last evaluated anchor {last_anchor}: {last_conf[1]}\n",
    "            Optimistic slope from last segment: {optimistic_slope}\n",
    "            Remaining steps: {(target_anchor - last_anchor)}\n",
    "            Estimated interval at target anchor {target_anchor} (pessimistic, optimistic): {estimate_for_target_performance}\"\"\")\n",
    "            return np.nan, normal_estimates_last[\"mean\"], estimates, elm\n",
    "\n",
    "        elif not enforce_all_anchor_evaluations and (elm.get_mean_performance_at_anchor(s_t) > r or (t >= 3 and elm.get_lc_estimate_at_target(target_anchor) >= r - MAX_ESTIMATE_MARGIN_FOR_FULL_EVALUATION)):\n",
    "            t = T\n",
    "            if (elm.get_mean_performance_at_anchor(s_t) > r):\n",
    "                logger.info(f\"Current mean is {elm.get_mean_performance_at_anchor(s_t)}, which is already an improvement over r = {r}. Hence, stepping to full anchor.\")\n",
    "            else:\n",
    "                logger.info(f\"Candidate appears to be competitive (predicted performance at {target_anchor} is {elm.get_lc_estimate_at_target(target_anchor)}. Jumping to last anchor in schedule: {t}\")\n",
    "        else:\n",
    "            t += 1\n",
    "            logger.info(f\"Finished schedule on {s_t}, and t is now {t}. Performance: {elm.get_normal_estimates(s_t, 4)}.\")\n",
    "            if t < T:\n",
    "                estimates = elm.get_normal_estimates()\n",
    "                logger.debug(\n",
    "                    \"LC: \"\n",
    "                    ''.join([\"\\n\\t\" + str(s_t) + \": \" + (str(estimates[s_t]) if s_t in estimates else \"n/a\") + \". Avg. runtime: \" + str(np.round(elm.get_runtimes_at_anchor(s_t).mean() / 1000, 1)) for s_t in schedule if len(elm.get_runtimes_at_anchor(s_t)) > 0])\n",
    "                )\n",
    "                if t > 2:\n",
    "                    logger.debug(\n",
    "                        f\"Estimate for target anchor {target_anchor}:\"\n",
    "                        f\"{elm.get_performance_interval_at_target(target_anchor)[1]}\"\n",
    "                    )\n",
    "    \n",
    "    # output final reports\n",
    "    toc = time.time()\n",
    "    estimates = elm.get_normal_estimates()\n",
    "    logger.info(f\"\"\"Learning Curve Construction Completed. Summary:\n",
    "    Runtime: {int(1000*(toc-tic))}ms.\n",
    "    LC: \"\"\"\n",
    "                ''.join([\"\\n\\t\\t\" + str(s_t) + \":\\t\" + (\", \".join([str(k) + \": \" + str(np.round(v, 4)) for k, v in estimates[s_t].items()]) if s_t in estimates else \"n/a\") + \". Avg. runtime: \" + str(np.round(elm.get_runtimes_at_anchor(s_t).mean(), 1)) for s_t in schedule if len(elm.get_runtimes_at_anchor(s_t)) > 0])\n",
    "                )\n",
    "    \n",
    "    # return result depending on observations and configuration\n",
    "    if len(estimates) == 0 or elm.get_best_worst_train_score() < r:\n",
    "        logger.info(f\"Observed no result or a train performance that is worse than r. In either case, returning nan.\")\n",
    "        return np.nan, np.nan, dict() if len(estimates) == 0 else estimates, elm\n",
    "    elif len(estimates) < 3:\n",
    "        max_anchor = max([int(k) for k in estimates])\n",
    "        if visualize_lcs:\n",
    "            logger.debug(f\"Visualizing curve\")\n",
    "            elm.visualize(schedule[-1], r)\n",
    "        return estimates[max_anchor][\"mean\"], estimates[max_anchor][\"mean\"], estimates, elm\n",
    "    else:\n",
    "        max_anchor = max([int(k) for k in estimates])\n",
    "        target_performance = estimates[max_anchor][\"mean\"] if t == T or not return_estimate_on_incomplete_runs else elm.get_lc_estimate_at_target(target_anchor)\n",
    "        logger.info(f\"Target performance: {target_performance}\")\n",
    "        if visualize_lcs:\n",
    "            logger.debug(f\"Visualizing curve\")\n",
    "            elm.visualize(schedule[-1], r)\n",
    "        return target_performance, estimates[max_anchor][\"mean\"], estimates, elm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
